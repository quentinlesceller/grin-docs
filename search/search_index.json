{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Grin Documentation Introduction to Mimblewimble and Grin Mimblewimble is a blockchain format and protocol that provides extremely good scalability, privacy and fungibility by relying on strong cryptographic primitives. It addresses gaps existing in almost all current blockchain implementations. Grin is an open source software project that implements a Mimblewimble blockchain and fills the gaps required for a full blockchain and cryptocurrency deployment. Here's an overview: Clean and minimal implementation, and aiming to stay as such. Follows the MimbleWimble protocol, which provides great anonymity and scaling characteristics. Cuckoo Cycle proof of work. Relatively fast block time: one minute. Fixed block reward over time with a decreasing dilution. Transaction fees are based on the number of Outputs created/destroyed and total transaction size. Smooth curve for difficulty adjustments. Goal and Characteristics Privacy by Default This enables complete fungibility without precluding the ability to selectively disclose information as needed. Scales mostly with the number of users and minimally with the number of transactions (<100 byte kernel), resulting in a large space saving compared to other blockchains. Strong and proven cryptography Mimblewimble only relies on Elliptic Curve Cryptography which has been tried and tested for decades. Design simplicity Easy to audit and maintain over time Community driven Community driven, encouraging mining decentralization. No ICO, Founder reward, pre-mine or aidrop.","title":"Home"},{"location":"#welcome-to-the-grin-documentation","text":"","title":"Welcome to the Grin Documentation"},{"location":"#introduction-to-mimblewimble-and-grin","text":"Mimblewimble is a blockchain format and protocol that provides extremely good scalability, privacy and fungibility by relying on strong cryptographic primitives. It addresses gaps existing in almost all current blockchain implementations. Grin is an open source software project that implements a Mimblewimble blockchain and fills the gaps required for a full blockchain and cryptocurrency deployment. Here's an overview: Clean and minimal implementation, and aiming to stay as such. Follows the MimbleWimble protocol, which provides great anonymity and scaling characteristics. Cuckoo Cycle proof of work. Relatively fast block time: one minute. Fixed block reward over time with a decreasing dilution. Transaction fees are based on the number of Outputs created/destroyed and total transaction size. Smooth curve for difficulty adjustments.","title":"Introduction to Mimblewimble and Grin"},{"location":"#goal-and-characteristics","text":"","title":"Goal and Characteristics"},{"location":"#privacy-by-default","text":"This enables complete fungibility without precluding the ability to selectively disclose information as needed. Scales mostly with the number of users and minimally with the number of transactions (<100 byte kernel), resulting in a large space saving compared to other blockchains.","title":"Privacy by Default"},{"location":"#strong-and-proven-cryptography","text":"Mimblewimble only relies on Elliptic Curve Cryptography which has been tried and tested for decades.","title":"Strong and proven cryptography"},{"location":"#design-simplicity","text":"Easy to audit and maintain over time","title":"Design simplicity"},{"location":"#community-driven","text":"Community driven, encouraging mining decentralization. No ICO, Founder reward, pre-mine or aidrop.","title":"Community driven"},{"location":"example/","text":"Full example from sanic import Sanic from sanic.response import json from sanic_camelcase_middleware import Camelize app = Sanic(__name__) Camelize(app) @app.route(\"/post\", methods=[\"POST\"]) async def test(request): return json(\"is_camelcase\": True, \"message\": request.json}) if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", port=8000)","title":"Full example"},{"location":"example/#full-example","text":"from sanic import Sanic from sanic.response import json from sanic_camelcase_middleware import Camelize app = Sanic(__name__) Camelize(app) @app.route(\"/post\", methods=[\"POST\"]) async def test(request): return json(\"is_camelcase\": True, \"message\": request.json}) if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", port=8000)","title":"Full example"},{"location":"getting-started/","text":"Getting Started Full code on github . Install pip install sanic_camelcase_middelware Dependencies pyhumps sanic Example from sanic import Sanic from sanic_camelcase_middleware import Camelize app = Sanic(__name__) Camelize(app)","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Full code on github .","title":"Getting Started"},{"location":"getting-started/#install","text":"pip install sanic_camelcase_middelware","title":"Install"},{"location":"getting-started/#dependencies","text":"pyhumps sanic","title":"Dependencies"},{"location":"getting-started/#example","text":"from sanic import Sanic from sanic_camelcase_middleware import Camelize app = Sanic(__name__) Camelize(app)","title":"Example"},{"location":"integrations/intro/","text":"","title":"Intro"},{"location":"technical/grin-for-bitcoiners/","text":"Grin for Bitcoiners Privacy and Fungibility There are 3 main properties of Grin transactions that make them private: There are no addresses. There are no amounts. 2 transactions, one spending the other, can be merged in a block to form only one, removing all intermediary information. The 2 first properties mean that all transactions are indistinguishable from one another. Unless you directly participated in the transaction, all inputs and outputs look like random pieces of data (in lingo, they're all random curve points). Moreover, there are no more transactions in a block. A Grin block looks just like one giant transaction and all original association between inputs and outputs is lost. Scalability As explained in the previous section, thanks to the Mimblewimble transaction and block format we can merge transactions when an output is directly spent by the input of another. It's as if when Alice gives money to Bob, and then Bob gives it all to Carol, Bob was never involved and his transaction is actually never even seen on the blockchain. Pushing that further, between blocks, most outputs end up being spent sooner or later by another input. So all spent outputs can be safely removed . And the whole blockchain can be stored, downloaded and fully verified in just a few gigabytes or less (assuming a number of transactions similar to bitcoin). This means that the Grin blockchain scales with the number of users (unspent outputs), not the number of transactions. At the moment, there is one caveat to that: a small piece of data (called a kernel , about 100 bytes) needs to stay around for each transaction. But we're working on optimizing that as well. Scripting Maybe you've heard that Mimblewimble doesn't support scripts. And in some way, that's true. But thanks to cryptographic trickery, many contracts that in Bitcoin would require a script can be achieved with Grin using properties of Elliptic Curve Cryptography. So far, we know how to do: Multi-signature transactions. Atomic swaps. Time-locked transactions and outputs. Lightning Network Emission Rate Bitcoin's 10 minute block time has its initial 50 btc reward cut in half every 4 years until there are 21 million bitcoin in circulation. Grin's emission rate is linear, meaning it never drops. The block reward is currently set at 60 grin with a block goal of 60 seconds. This still works because 1) dilution trends toward zero and 2) a non-negligible amount of coins gets lost or destroyed every year. FAQ Wait, what!? No address? Nope, no address. All outputs in Grin are unique and have no common data with any previous output. Instead of relying on a known address to send money, transactions have to be built interactively, with two (or more) wallets exchanging data with one another. This interaction does not require both parties to be online at the same time . Practically speaking, there are many ways for two programs to interact privately and securely. This interaction could even take place over email or Signal (or carrier pigeons). If transaction information gets removed, can I just cheat and create money? No, and this is where Mimblewimble and Grin shine. Confidential transactions are a form of homomorphic encryption . Without revealing any amount, Grin can verify that the sum of all transaction inputs equal the sum of transaction outputs, plus the fee. Going even further, comparing the sum of all money created by mining with the total sum of money that's being held, Grin nodes can check the correctness of the total money supply. If I listen to transaction relay, can't I just figure out who they belong to before being cut-through? You can figure out which outputs are being spent by which transaction, but the trail of data stops here. All inputs and outputs look like random pieces of data, so you can't tell if the money was transferred, still belongs to the same person, which output is the actual transfer and which is the change, etc. Grin transactions are built with no identifiable piece of information . In addition, Grin leverages Dandelion relay , which provides additional anonymity as to which IP or client the transaction originated from, and allows for transactions to be aggregated. What about the quantum computaggedon? In every Grin output, we also include a bit of hashed data, which is quantum safe. If quantum computing was to become a reality, we can safely introduce additional verification that would protect existing coins from being hacked. How does all this magic work? See our technical introduction to get started.","title":"Grin for Bitcoiners"},{"location":"technical/grin-for-bitcoiners/#grin-for-bitcoiners","text":"","title":"Grin for Bitcoiners"},{"location":"technical/grin-for-bitcoiners/#privacy-and-fungibility","text":"There are 3 main properties of Grin transactions that make them private: There are no addresses. There are no amounts. 2 transactions, one spending the other, can be merged in a block to form only one, removing all intermediary information. The 2 first properties mean that all transactions are indistinguishable from one another. Unless you directly participated in the transaction, all inputs and outputs look like random pieces of data (in lingo, they're all random curve points). Moreover, there are no more transactions in a block. A Grin block looks just like one giant transaction and all original association between inputs and outputs is lost.","title":"Privacy and Fungibility"},{"location":"technical/grin-for-bitcoiners/#scalability","text":"As explained in the previous section, thanks to the Mimblewimble transaction and block format we can merge transactions when an output is directly spent by the input of another. It's as if when Alice gives money to Bob, and then Bob gives it all to Carol, Bob was never involved and his transaction is actually never even seen on the blockchain. Pushing that further, between blocks, most outputs end up being spent sooner or later by another input. So all spent outputs can be safely removed . And the whole blockchain can be stored, downloaded and fully verified in just a few gigabytes or less (assuming a number of transactions similar to bitcoin). This means that the Grin blockchain scales with the number of users (unspent outputs), not the number of transactions. At the moment, there is one caveat to that: a small piece of data (called a kernel , about 100 bytes) needs to stay around for each transaction. But we're working on optimizing that as well.","title":"Scalability"},{"location":"technical/grin-for-bitcoiners/#scripting","text":"Maybe you've heard that Mimblewimble doesn't support scripts. And in some way, that's true. But thanks to cryptographic trickery, many contracts that in Bitcoin would require a script can be achieved with Grin using properties of Elliptic Curve Cryptography. So far, we know how to do: Multi-signature transactions. Atomic swaps. Time-locked transactions and outputs. Lightning Network","title":"Scripting"},{"location":"technical/grin-for-bitcoiners/#emission-rate","text":"Bitcoin's 10 minute block time has its initial 50 btc reward cut in half every 4 years until there are 21 million bitcoin in circulation. Grin's emission rate is linear, meaning it never drops. The block reward is currently set at 60 grin with a block goal of 60 seconds. This still works because 1) dilution trends toward zero and 2) a non-negligible amount of coins gets lost or destroyed every year.","title":"Emission Rate"},{"location":"technical/grin-for-bitcoiners/#faq","text":"","title":"FAQ"},{"location":"technical/grin-for-bitcoiners/#wait-what-no-address","text":"Nope, no address. All outputs in Grin are unique and have no common data with any previous output. Instead of relying on a known address to send money, transactions have to be built interactively, with two (or more) wallets exchanging data with one another. This interaction does not require both parties to be online at the same time . Practically speaking, there are many ways for two programs to interact privately and securely. This interaction could even take place over email or Signal (or carrier pigeons).","title":"Wait, what!? No address?"},{"location":"technical/grin-for-bitcoiners/#if-transaction-information-gets-removed-can-i-just-cheat-and-create-money","text":"No, and this is where Mimblewimble and Grin shine. Confidential transactions are a form of homomorphic encryption . Without revealing any amount, Grin can verify that the sum of all transaction inputs equal the sum of transaction outputs, plus the fee. Going even further, comparing the sum of all money created by mining with the total sum of money that's being held, Grin nodes can check the correctness of the total money supply.","title":"If transaction information gets removed, can I just cheat and create money?"},{"location":"technical/grin-for-bitcoiners/#if-i-listen-to-transaction-relay-cant-i-just-figure-out-who-they-belong-to-before-being-cut-through","text":"You can figure out which outputs are being spent by which transaction, but the trail of data stops here. All inputs and outputs look like random pieces of data, so you can't tell if the money was transferred, still belongs to the same person, which output is the actual transfer and which is the change, etc. Grin transactions are built with no identifiable piece of information . In addition, Grin leverages Dandelion relay , which provides additional anonymity as to which IP or client the transaction originated from, and allows for transactions to be aggregated.","title":"If I listen to transaction relay, can't I just figure out who they belong to before being cut-through?"},{"location":"technical/grin-for-bitcoiners/#what-about-the-quantum-computaggedon","text":"In every Grin output, we also include a bit of hashed data, which is quantum safe. If quantum computing was to become a reality, we can safely introduce additional verification that would protect existing coins from being hacked.","title":"What about the quantum computaggedon?"},{"location":"technical/grin-for-bitcoiners/#how-does-all-this-magic-work","text":"See our technical introduction to get started.","title":"How does all this magic work?"},{"location":"technical/introduction-to-mimblewimble/","text":"Technical Introduction to Mimblewimble Tongue Tying for Everyone This document is targeted at readers with a good understanding of blockchains and basic cryptography. With that in mind, we attempt to explain the technical buildup of Mimblewimble and how it's applied in Grin. We hope this document is understandable to most technically-minded readers. Our objective is to encourage you to get interested in Grin and contribute in any way possible. To achieve this objective, we will introduce the main concepts required for a good understanding of Grin as a Mimblewimble implementation. We will start with a brief description of some relevant properties of Elliptic Curve Cryptography (ECC) to lay the foundation on which Grin is based and then describe all the key elements of a Mimblewimble blockchain's transactions and blocks. Tiny Bits of Elliptic Curves We start with a brief primer on Elliptic Curve Cryptography, reviewing just the properties necessary to understand how Mimblewimble works and without delving too much into the intricacies of ECC. For readers who would want to dive deeper into those assumptions, there are other opportunities to learn more . An Elliptic Curve for the purpose of cryptography is simply a large set of points that we will call C . These points can be added, subtracted, or multiplied by integers (also called scalars). Given such a point H , an integer k and using the scalar multiplication operation we can compute k*H , which is also a point on curve C . Given another integer j we can also calculate (k+j)*H , which equals k*H + j*H . The addition and scalar multiplication operations on an elliptic curve maintain the commutative and associative properties of addition and multiplication: (k+j)*H = k*H + j*H In ECC, if we pick a very large number k as a private key, k*H is considered the corresponding public key. Even if one knows the value of the public key k*H , deducing k is close to impossible (or said differently, while multiplication is trivial, \"division\" by curve points is extremely difficult). The previous formula (k+j)*H = k*H + j*H , with k and j both private keys, demonstrates that a public key obtained from the addition of two private keys ( (k+j)*H ) is identical to the addition of the public keys for each of those two private keys ( k*H + j*H ). In the Bitcoin blockchain, Hierarchical Deterministic wallets heavily rely on this principle. Mimblewimble and the Grin implementation do as well. Transacting with Mimblewimble The structure of transactions demonstrates a crucial tenet of Mimblewimble: strong privacy and confidentiality guarantees. The validation of Mimblewimble transactions relies on two basic properties: Verification of zero sums. The sum of outputs minus inputs always equals zero, proving that the transaction did not create new funds, without revealing the actual amounts . Possession of private keys. Like with most other cryptocurrencies, ownership of transaction outputs is guaranteed by the possession of ECC private keys. However, the proof that an entity owns those private keys is not achieved by directly signing the transaction. The next sections on balance, ownership, change and proofs details how those two fundamental properties are achieved. Balance Building upon the properties of ECC we described above, one can obscure the values in a transaction. If v is the value of a transaction input or output and H a point on the elliptic curve C , we can simply embed v*H instead of v in a transaction. This works because using the ECC operations, we can still validate that the sum of the outputs of a transaction equals the sum of inputs: v1 + v2 = v3 => v1*H + v2*H = v3*H Verifying this property on every transaction allows the protocol to verify that a transaction doesn't create money out of thin air, without knowing what the actual values are. However, there are a finite number of usable values (transaction amounts) and one could try every single one of them to guess the value of the transaction. In addition, knowing v1 (from a previous transaction for example) and the resulting v1*H reveals all outputs with value v1 across the blockchain. For these reasons, we introduce a second point G on the same elliptic curve (practically G is just another generator point on the same curve group as H ) and a private key r used as a blinding factor . An input or output value in a transaction can then be expressed as: r*G + v*H Where: r is a private key used as a blinding factor, G is a point on the elliptic curve C and their product r*G is the public key for r (using G as generator point). v is the value of an input or output and H is another point on the elliptic curve C , together producing another public key v*H (using H as generator point). Neither v nor r can be deduced, leveraging the fundamental properties of Elliptic Curve Cryptography. r*G + v*H is called a Pedersen Commitment . As an example, let's assume we want to build a transaction with two inputs and one output. We have (ignoring fees): vi1 and vi2 as input values. vo3 as output value. Such that: vi1 + vi2 = vo3 Generating a private key as a blinding factor for each input value and replacing each value with their respective Pedersen Commitments in the previous equation, we obtain: (ri1*G + vi1*H) + (ri2*G + vi2*H) = (ro3*G + vo3*H) Which as a consequence requires that: ri1 + ri2 = ro3 This is the first pillar of Mimblewimble: the arithmetic required to validate a transaction can be done without knowing any of the values. As a final note, this idea is actually derived from Greg Maxwell's Confidential Transactions , which is itself derived from an Adam Back proposal for homomorphic values applied to Bitcoin. Ownership In the previous section we introduced a private key as a blinding factor to obscure the transaction's values. The second insight of Mimblewimble is that this private key can be leveraged to prove ownership of the value. Alice sends you 3 coins and to obscure that amount, you chose 28 as your blinding factor (note that in practice, the blinding factor being a private key, it's an extremely large number). Somewhere on the blockchain, the following output appears and should only be spendable by you: X = 28*G + 3*H X , the result of the addition, is visible by everyone. The value 3 is only known to you and Alice, and 28 is only known to you. To transfer those 3 coins again, the protocol requires 28 to be known somehow. To demonstrate how this works, let's say you want to transfer those 3 same coins to Carol. You need to build a simple transaction such that: Xi => Y Where Xi is an input that spends your X output and Y is Carol's output. There is no way to build such a transaction and balance it without knowing your private key of 28. Indeed, if Carol is to balance this transaction, she needs to know both the value sent and your private key so that: Y - Xi = (28*G + 3*H) - (28*G + 3*H) = 0*G + 0*H By checking that everything has been zeroed out, we can again make sure that no new money has been created. Wait! Stop! Now you know the private key in Carol's output (which, in this case, must be the same as yours to balance out) and so you could steal the money back from Carol! To solve this, Carol uses a private key of her choosing. She picks 113 say, and what ends up on the blockchain is: Y - Xi = (113*G + 3*H) - (28*G + 3*H) = 85*G + 0*H Now the transaction no longer sums to zero and we have an excess value (85), which is the result of the summation of all blinding factors. Because 85*G is a valid public key for the generator point G the input and output values must sum to zero and the transaction is thus valid, since x*G + y*H is a valid public key for generator point G if and only if y = 0 . So all the protocol needs to verify is that ( Y - Xi ) is a valid public key for generator point G and that the transacting parties collectively can produce its private key (85 in the above example). The simplest way to do so is to require a signature built with the excess value (85), which then ensures that: The transacting parties collectively can produce the private key (the excess value) The sum of the outputs minus the inputs are zero (because only a valid public key will check against the signature). This signature, attached to every transaction, together with some additional data (like mining fees), is called a transaction kernel and is checked by all validators. Some Finer Points This section elaborates on the building of transactions by discussing how change is introduced and the requirement for range proofs so all values are proven to be non-negative. Neither of these are absolutely required to understand Mimblewimble and Grin, so if you're in a hurry, feel free to jump straight to Putting It All Together . Change Let's say you only want to send 2 coins to Carol from the 3 you received from Alice. To do this you would send the remaining 1 coin back to yourself as change. You generate another private key (say 12) as a blinding factor to protect your change output. Carol uses her own private key as before. Change output: 12*G + 1*H Carol's output: 113*G + 2*H What ends up on the blockchain is something very similar to before. And the signature is again built with the excess value, 97 in this example. (12*G + 1*H) + (113*G + 2*H) - (28*G + 3*H) = 97*G + 0*H Range Proofs In all the above calculations, we rely on the transaction values to always be positive. The introduction of negative amounts would be extremely problematic as one could create new funds in every transaction. For example, one could create a transaction with an input of 2 and outputs of 5 and -3 and still obtain a well-balanced transaction. This can't be easily detected because even if x is negative, the corresponding point x*H on the curve looks like any other. To solve this problem, Mimblewimble leverages another cryptographic concept (also coming from Confidential Transactions) called range proofs: a proof that a number falls within a given range, without revealing the number. We won't elaborate on the range proof, but you just need to know that for any r*G + v*H we can build a proof that will show that v is greater than zero and does not overflow. It's also important to note that range proofs for both the blinding factor and the values are needed. The reason for this is that it prevents a censoring attack where a third party would be able to lock UTXOs without knowing their private keys by creating a transaction such as the following: Carol's UTXO: 113*G + 2*H Attacker's output: (113 + 99)*G + 2*H which can be signed by the attacker because Carol's blinding factor cancels out in the equation Y - Xi : Y - Xi = ((113 + 99)*G + 2*H) - (113*G + 2*H) = 99*G This output ( (113 + 99)*G + 2*H ) requires that both the numbers 113 and 99 are known in order to be spent; the attacker would thus have successfully locked Carol's UTXO. The requirement for a range proof for the blinding factor prevents this because the attacker doesn't know the number 113 and thus neither (113 + 99). A more detailed description of range proofs is further detailed in the range proof paper . Putting It All Together A Mimblewimble transaction includes the following: A set of inputs, that reference and spend a set of previous outputs. A set of new outputs that include: A value and a blinding factor (which is just a new private key) multiplied on a curve and summed to be r*G + v*H . A range proof that among other things shows that v is non-negative. An transaction fee in cleartext. A signature whose private key is computed by taking the excess value (the sum of all output values plus the fee, minus the input values). Blocks and Chain State We explained above how Mimblewimble transactions can provide strong anonymity guarantees while maintaining the properties required for a valid blockchain, i.e., a transaction does not create money and proof of ownership is established through private keys. The Mimblewimble block format builds on this by introducing one additional concept: cut-through . With this addition, a Mimblewimble chain gains: Extremely good scalability, as the great majority of transaction data can be eliminated over time, without compromising security. Further anonymity by mixing and removing transaction data. Transaction Aggregation Recall that a transaction consists of the following: a set of inputs that reference and spent a set of previous outputs a set of new outputs a transaction kernel consisting of: kernel excess (the public key of the excess value) transaction signature whose public key is the kernel excess A transaction is validated by determining that the kernel excess is a valid public key: (42*G + 1*H) + (99*G + 2*H) - (113*G + 3*H) = 28*G + 0*H The public key in this example is 28*G . We can say the following is true for any valid transaction (ignoring fees for simplicity): sum(outputs) - sum(inputs) = kernel_excess The same holds true for blocks themselves once we realize a block is simply a set of aggregated inputs, outputs and transaction kernels. We can sum the outputs, subtract the inputs from it and equating the resulting Pedersen commitment to the sum of the kernel excesses: sum(outputs) - sum(inputs) = sum(kernel_excess) Simplifying slightly, (again ignoring transaction fees) we can say that Mimblewimble blocks can be treated exactly as Mimblewimble transactions. Kernel Offsets There is a subtle problem with Mimblewimble blocks and transactions as described above. It is possible (and in some cases trivial) to reconstruct the constituent transactions in a block. This is clearly bad for privacy. This is the \"subset\" problem: given a set of inputs, outputs, and transaction kernels a subset of these will recombine to reconstruct a valid transaction. Consider the following two transactions: (in1, in2) -> (out1), (kern1) (in3) -> (out2), (kern2) We can aggregate them into the following block (or aggregate transaction): (in1, in2, in3) -> (out1, out2), (kern1, kern2) It is trivially easy to try all possible permutations to recover one of the transactions (where it successfully sums to zero): (in1, in2) -> (out1), (kern1) We also know that everything remaining can be used to reconstruct the other valid transaction: (in3) -> (out2), (kern2) Remember that the kernel excess r*G simply is the public key of the excess value r . To mitigate this we redefine the kernel excess from r*G to (r-kernel_offset)*G and distribute the kernel offset to be included with every transaction kernel. The kernel offset is thus a blinding factor that needs to be added to the excess value to ensure the commitments sum to zero: sum(outputs) - sum(inputs) = r*G = (r-kernel_offset)*G + kernel_offset*G or alternatively sum(outputs) - sum(inputs) = kernel_excess + kernel_offset*G For a commitment r*G + 0*H with the offset a , the transaction is signed with (r-a) and a is published so that r*G can be calculated in order to verify the validity of the transaction. During block construction all kernel offsets are summed to generate a single aggregate kernel offset to cover the whole block. The kernel offset for any individual transaction is then unrecoverable and the subset problem is solved. sum(outputs) - sum(inputs) = sum(kernel_excess) + kernel_offset*G Cut-through Blocks let miners assemble multiple transactions into a single set that's added to the chain. In the following block representations, containing 3 transactions, we only show inputs and outputs of transactions. Inputs reference outputs they spend. An output included in a previous block is marked with a lower-case x. I1(x1) --- O1 |- O2 I2(x2) --- O3 I3(O2) -| I4(O3) --- O4 |- O5 We notice the two following properties: Within this block, some outputs are directly spent by following inputs ( I3 spends O2 and I4 spends O3 ). The structure of each transaction does not actually matter. Since all transactions individually sum to zero, the sum of all transaction inputs and outputs must be zero. Similarly to a transaction, all that needs to be checked in a block is that ownership has been proven (which comes from the transaction kernels ) and that the whole block did not create any coins (other than what's allowed as the mining reward). Therefore, matching inputs and outputs can be eliminated, as their contribution to the overall sum cancels out. Which leads to the following, much more compact block: I1(x1) | O1 I2(x2) | O4 | O5 Note that all transaction structure has been eliminated and the order of inputs and outputs does not matter anymore. However, the sum of all inputs and outputs is still guaranteed to be zero. A block is simply built from: A block header. The list of inputs remaining after cut-through. The list of outputs remaining after cut-through. A single kernel offset to cover the full block. The transaction kernels containing, for each transaction: The public key r*G obtained from the summation of all inputs and outputs. The signatures generated using the excess value. The mining fee. When structured this way, a Mimblewimble block offers extremely good privacy guarantees: Intermediate (cut-through) transactions will be represented only by their transaction kernels. All outputs look the same: very large numbers that are impossible to meaningfully differentiate from one another. If someone wants to exclude a specific output, they'd have to exclude all. All transaction structure has been removed, making it impossible to tell which inputs and outputs match. And yet, it all still validates! Cut-through All The Way Going back to the previous example block, outputs x1 and x2 , spent by I1 and I2 , must have appeared previously in the blockchain. So after the addition of this block, those outputs as well as I1 and I2 can also be removed from the blockchain as they now are intermediate transactions. We conclude that the chain state (excluding headers) at any point in time can be summarized by just these pieces of information: The total amount of coins created by mining in the chain. The complete set of unspent outputs. The transactions kernels for each transaction. The first piece of information can be deduced just using the block height. Both the UTXOs and the transaction kernels are extremely compact. This has two important consequences: The blockchain a node needs to maintain is very small (on the order of a few gigabytes for a bitcoin-sized blockchain, and potentially optimizable to a few hundreds of megabytes). When a new node joins the network the amount of information that needs to be transferred is very small. In addition, the UTXO set cannot be tampered with. Adding or removing even one input or output would change the sum of the transactions to be something other than zero. Conclusion In this document we covered the basic principles that underlie a Mimblewimble blockchain. By using the addition properties of Elliptic Curve Cryptography, we're able to build transactions that are completely opaque but can still be properly validated. And by generalizing those properties to blocks, we can eliminate a large amount of blockchain data, allowing for great scaling and fast sync of new peers.","title":"Introduction to Mimblewimble"},{"location":"technical/introduction-to-mimblewimble/#technical-introduction-to-mimblewimble","text":"","title":"Technical Introduction to Mimblewimble"},{"location":"technical/introduction-to-mimblewimble/#tongue-tying-for-everyone","text":"This document is targeted at readers with a good understanding of blockchains and basic cryptography. With that in mind, we attempt to explain the technical buildup of Mimblewimble and how it's applied in Grin. We hope this document is understandable to most technically-minded readers. Our objective is to encourage you to get interested in Grin and contribute in any way possible. To achieve this objective, we will introduce the main concepts required for a good understanding of Grin as a Mimblewimble implementation. We will start with a brief description of some relevant properties of Elliptic Curve Cryptography (ECC) to lay the foundation on which Grin is based and then describe all the key elements of a Mimblewimble blockchain's transactions and blocks.","title":"Tongue Tying for Everyone"},{"location":"technical/introduction-to-mimblewimble/#tiny-bits-of-elliptic-curves","text":"We start with a brief primer on Elliptic Curve Cryptography, reviewing just the properties necessary to understand how Mimblewimble works and without delving too much into the intricacies of ECC. For readers who would want to dive deeper into those assumptions, there are other opportunities to learn more . An Elliptic Curve for the purpose of cryptography is simply a large set of points that we will call C . These points can be added, subtracted, or multiplied by integers (also called scalars). Given such a point H , an integer k and using the scalar multiplication operation we can compute k*H , which is also a point on curve C . Given another integer j we can also calculate (k+j)*H , which equals k*H + j*H . The addition and scalar multiplication operations on an elliptic curve maintain the commutative and associative properties of addition and multiplication: (k+j)*H = k*H + j*H In ECC, if we pick a very large number k as a private key, k*H is considered the corresponding public key. Even if one knows the value of the public key k*H , deducing k is close to impossible (or said differently, while multiplication is trivial, \"division\" by curve points is extremely difficult). The previous formula (k+j)*H = k*H + j*H , with k and j both private keys, demonstrates that a public key obtained from the addition of two private keys ( (k+j)*H ) is identical to the addition of the public keys for each of those two private keys ( k*H + j*H ). In the Bitcoin blockchain, Hierarchical Deterministic wallets heavily rely on this principle. Mimblewimble and the Grin implementation do as well.","title":"Tiny Bits of Elliptic Curves"},{"location":"technical/introduction-to-mimblewimble/#transacting-with-mimblewimble","text":"The structure of transactions demonstrates a crucial tenet of Mimblewimble: strong privacy and confidentiality guarantees. The validation of Mimblewimble transactions relies on two basic properties: Verification of zero sums. The sum of outputs minus inputs always equals zero, proving that the transaction did not create new funds, without revealing the actual amounts . Possession of private keys. Like with most other cryptocurrencies, ownership of transaction outputs is guaranteed by the possession of ECC private keys. However, the proof that an entity owns those private keys is not achieved by directly signing the transaction. The next sections on balance, ownership, change and proofs details how those two fundamental properties are achieved.","title":"Transacting with Mimblewimble"},{"location":"technical/introduction-to-mimblewimble/#balance","text":"Building upon the properties of ECC we described above, one can obscure the values in a transaction. If v is the value of a transaction input or output and H a point on the elliptic curve C , we can simply embed v*H instead of v in a transaction. This works because using the ECC operations, we can still validate that the sum of the outputs of a transaction equals the sum of inputs: v1 + v2 = v3 => v1*H + v2*H = v3*H Verifying this property on every transaction allows the protocol to verify that a transaction doesn't create money out of thin air, without knowing what the actual values are. However, there are a finite number of usable values (transaction amounts) and one could try every single one of them to guess the value of the transaction. In addition, knowing v1 (from a previous transaction for example) and the resulting v1*H reveals all outputs with value v1 across the blockchain. For these reasons, we introduce a second point G on the same elliptic curve (practically G is just another generator point on the same curve group as H ) and a private key r used as a blinding factor . An input or output value in a transaction can then be expressed as: r*G + v*H Where: r is a private key used as a blinding factor, G is a point on the elliptic curve C and their product r*G is the public key for r (using G as generator point). v is the value of an input or output and H is another point on the elliptic curve C , together producing another public key v*H (using H as generator point). Neither v nor r can be deduced, leveraging the fundamental properties of Elliptic Curve Cryptography. r*G + v*H is called a Pedersen Commitment . As an example, let's assume we want to build a transaction with two inputs and one output. We have (ignoring fees): vi1 and vi2 as input values. vo3 as output value. Such that: vi1 + vi2 = vo3 Generating a private key as a blinding factor for each input value and replacing each value with their respective Pedersen Commitments in the previous equation, we obtain: (ri1*G + vi1*H) + (ri2*G + vi2*H) = (ro3*G + vo3*H) Which as a consequence requires that: ri1 + ri2 = ro3 This is the first pillar of Mimblewimble: the arithmetic required to validate a transaction can be done without knowing any of the values. As a final note, this idea is actually derived from Greg Maxwell's Confidential Transactions , which is itself derived from an Adam Back proposal for homomorphic values applied to Bitcoin.","title":"Balance"},{"location":"technical/introduction-to-mimblewimble/#ownership","text":"In the previous section we introduced a private key as a blinding factor to obscure the transaction's values. The second insight of Mimblewimble is that this private key can be leveraged to prove ownership of the value. Alice sends you 3 coins and to obscure that amount, you chose 28 as your blinding factor (note that in practice, the blinding factor being a private key, it's an extremely large number). Somewhere on the blockchain, the following output appears and should only be spendable by you: X = 28*G + 3*H X , the result of the addition, is visible by everyone. The value 3 is only known to you and Alice, and 28 is only known to you. To transfer those 3 coins again, the protocol requires 28 to be known somehow. To demonstrate how this works, let's say you want to transfer those 3 same coins to Carol. You need to build a simple transaction such that: Xi => Y Where Xi is an input that spends your X output and Y is Carol's output. There is no way to build such a transaction and balance it without knowing your private key of 28. Indeed, if Carol is to balance this transaction, she needs to know both the value sent and your private key so that: Y - Xi = (28*G + 3*H) - (28*G + 3*H) = 0*G + 0*H By checking that everything has been zeroed out, we can again make sure that no new money has been created. Wait! Stop! Now you know the private key in Carol's output (which, in this case, must be the same as yours to balance out) and so you could steal the money back from Carol! To solve this, Carol uses a private key of her choosing. She picks 113 say, and what ends up on the blockchain is: Y - Xi = (113*G + 3*H) - (28*G + 3*H) = 85*G + 0*H Now the transaction no longer sums to zero and we have an excess value (85), which is the result of the summation of all blinding factors. Because 85*G is a valid public key for the generator point G the input and output values must sum to zero and the transaction is thus valid, since x*G + y*H is a valid public key for generator point G if and only if y = 0 . So all the protocol needs to verify is that ( Y - Xi ) is a valid public key for generator point G and that the transacting parties collectively can produce its private key (85 in the above example). The simplest way to do so is to require a signature built with the excess value (85), which then ensures that: The transacting parties collectively can produce the private key (the excess value) The sum of the outputs minus the inputs are zero (because only a valid public key will check against the signature). This signature, attached to every transaction, together with some additional data (like mining fees), is called a transaction kernel and is checked by all validators.","title":"Ownership"},{"location":"technical/introduction-to-mimblewimble/#some-finer-points","text":"This section elaborates on the building of transactions by discussing how change is introduced and the requirement for range proofs so all values are proven to be non-negative. Neither of these are absolutely required to understand Mimblewimble and Grin, so if you're in a hurry, feel free to jump straight to Putting It All Together .","title":"Some Finer Points"},{"location":"technical/introduction-to-mimblewimble/#change","text":"Let's say you only want to send 2 coins to Carol from the 3 you received from Alice. To do this you would send the remaining 1 coin back to yourself as change. You generate another private key (say 12) as a blinding factor to protect your change output. Carol uses her own private key as before. Change output: 12*G + 1*H Carol's output: 113*G + 2*H What ends up on the blockchain is something very similar to before. And the signature is again built with the excess value, 97 in this example. (12*G + 1*H) + (113*G + 2*H) - (28*G + 3*H) = 97*G + 0*H","title":"Change"},{"location":"technical/introduction-to-mimblewimble/#range-proofs","text":"In all the above calculations, we rely on the transaction values to always be positive. The introduction of negative amounts would be extremely problematic as one could create new funds in every transaction. For example, one could create a transaction with an input of 2 and outputs of 5 and -3 and still obtain a well-balanced transaction. This can't be easily detected because even if x is negative, the corresponding point x*H on the curve looks like any other. To solve this problem, Mimblewimble leverages another cryptographic concept (also coming from Confidential Transactions) called range proofs: a proof that a number falls within a given range, without revealing the number. We won't elaborate on the range proof, but you just need to know that for any r*G + v*H we can build a proof that will show that v is greater than zero and does not overflow. It's also important to note that range proofs for both the blinding factor and the values are needed. The reason for this is that it prevents a censoring attack where a third party would be able to lock UTXOs without knowing their private keys by creating a transaction such as the following: Carol's UTXO: 113*G + 2*H Attacker's output: (113 + 99)*G + 2*H which can be signed by the attacker because Carol's blinding factor cancels out in the equation Y - Xi : Y - Xi = ((113 + 99)*G + 2*H) - (113*G + 2*H) = 99*G This output ( (113 + 99)*G + 2*H ) requires that both the numbers 113 and 99 are known in order to be spent; the attacker would thus have successfully locked Carol's UTXO. The requirement for a range proof for the blinding factor prevents this because the attacker doesn't know the number 113 and thus neither (113 + 99). A more detailed description of range proofs is further detailed in the range proof paper .","title":"Range Proofs"},{"location":"technical/introduction-to-mimblewimble/#putting-it-all-together","text":"A Mimblewimble transaction includes the following: A set of inputs, that reference and spend a set of previous outputs. A set of new outputs that include: A value and a blinding factor (which is just a new private key) multiplied on a curve and summed to be r*G + v*H . A range proof that among other things shows that v is non-negative. An transaction fee in cleartext. A signature whose private key is computed by taking the excess value (the sum of all output values plus the fee, minus the input values).","title":"Putting It All Together"},{"location":"technical/introduction-to-mimblewimble/#blocks-and-chain-state","text":"We explained above how Mimblewimble transactions can provide strong anonymity guarantees while maintaining the properties required for a valid blockchain, i.e., a transaction does not create money and proof of ownership is established through private keys. The Mimblewimble block format builds on this by introducing one additional concept: cut-through . With this addition, a Mimblewimble chain gains: Extremely good scalability, as the great majority of transaction data can be eliminated over time, without compromising security. Further anonymity by mixing and removing transaction data.","title":"Blocks and Chain State"},{"location":"technical/introduction-to-mimblewimble/#transaction-aggregation","text":"Recall that a transaction consists of the following: a set of inputs that reference and spent a set of previous outputs a set of new outputs a transaction kernel consisting of: kernel excess (the public key of the excess value) transaction signature whose public key is the kernel excess A transaction is validated by determining that the kernel excess is a valid public key: (42*G + 1*H) + (99*G + 2*H) - (113*G + 3*H) = 28*G + 0*H The public key in this example is 28*G . We can say the following is true for any valid transaction (ignoring fees for simplicity): sum(outputs) - sum(inputs) = kernel_excess The same holds true for blocks themselves once we realize a block is simply a set of aggregated inputs, outputs and transaction kernels. We can sum the outputs, subtract the inputs from it and equating the resulting Pedersen commitment to the sum of the kernel excesses: sum(outputs) - sum(inputs) = sum(kernel_excess) Simplifying slightly, (again ignoring transaction fees) we can say that Mimblewimble blocks can be treated exactly as Mimblewimble transactions.","title":"Transaction Aggregation"},{"location":"technical/introduction-to-mimblewimble/#kernel-offsets","text":"There is a subtle problem with Mimblewimble blocks and transactions as described above. It is possible (and in some cases trivial) to reconstruct the constituent transactions in a block. This is clearly bad for privacy. This is the \"subset\" problem: given a set of inputs, outputs, and transaction kernels a subset of these will recombine to reconstruct a valid transaction. Consider the following two transactions: (in1, in2) -> (out1), (kern1) (in3) -> (out2), (kern2) We can aggregate them into the following block (or aggregate transaction): (in1, in2, in3) -> (out1, out2), (kern1, kern2) It is trivially easy to try all possible permutations to recover one of the transactions (where it successfully sums to zero): (in1, in2) -> (out1), (kern1) We also know that everything remaining can be used to reconstruct the other valid transaction: (in3) -> (out2), (kern2) Remember that the kernel excess r*G simply is the public key of the excess value r . To mitigate this we redefine the kernel excess from r*G to (r-kernel_offset)*G and distribute the kernel offset to be included with every transaction kernel. The kernel offset is thus a blinding factor that needs to be added to the excess value to ensure the commitments sum to zero: sum(outputs) - sum(inputs) = r*G = (r-kernel_offset)*G + kernel_offset*G or alternatively sum(outputs) - sum(inputs) = kernel_excess + kernel_offset*G For a commitment r*G + 0*H with the offset a , the transaction is signed with (r-a) and a is published so that r*G can be calculated in order to verify the validity of the transaction. During block construction all kernel offsets are summed to generate a single aggregate kernel offset to cover the whole block. The kernel offset for any individual transaction is then unrecoverable and the subset problem is solved. sum(outputs) - sum(inputs) = sum(kernel_excess) + kernel_offset*G","title":"Kernel Offsets"},{"location":"technical/introduction-to-mimblewimble/#cut-through","text":"Blocks let miners assemble multiple transactions into a single set that's added to the chain. In the following block representations, containing 3 transactions, we only show inputs and outputs of transactions. Inputs reference outputs they spend. An output included in a previous block is marked with a lower-case x. I1(x1) --- O1 |- O2 I2(x2) --- O3 I3(O2) -| I4(O3) --- O4 |- O5 We notice the two following properties: Within this block, some outputs are directly spent by following inputs ( I3 spends O2 and I4 spends O3 ). The structure of each transaction does not actually matter. Since all transactions individually sum to zero, the sum of all transaction inputs and outputs must be zero. Similarly to a transaction, all that needs to be checked in a block is that ownership has been proven (which comes from the transaction kernels ) and that the whole block did not create any coins (other than what's allowed as the mining reward). Therefore, matching inputs and outputs can be eliminated, as their contribution to the overall sum cancels out. Which leads to the following, much more compact block: I1(x1) | O1 I2(x2) | O4 | O5 Note that all transaction structure has been eliminated and the order of inputs and outputs does not matter anymore. However, the sum of all inputs and outputs is still guaranteed to be zero. A block is simply built from: A block header. The list of inputs remaining after cut-through. The list of outputs remaining after cut-through. A single kernel offset to cover the full block. The transaction kernels containing, for each transaction: The public key r*G obtained from the summation of all inputs and outputs. The signatures generated using the excess value. The mining fee. When structured this way, a Mimblewimble block offers extremely good privacy guarantees: Intermediate (cut-through) transactions will be represented only by their transaction kernels. All outputs look the same: very large numbers that are impossible to meaningfully differentiate from one another. If someone wants to exclude a specific output, they'd have to exclude all. All transaction structure has been removed, making it impossible to tell which inputs and outputs match. And yet, it all still validates!","title":"Cut-through"},{"location":"technical/introduction-to-mimblewimble/#cut-through-all-the-way","text":"Going back to the previous example block, outputs x1 and x2 , spent by I1 and I2 , must have appeared previously in the blockchain. So after the addition of this block, those outputs as well as I1 and I2 can also be removed from the blockchain as they now are intermediate transactions. We conclude that the chain state (excluding headers) at any point in time can be summarized by just these pieces of information: The total amount of coins created by mining in the chain. The complete set of unspent outputs. The transactions kernels for each transaction. The first piece of information can be deduced just using the block height. Both the UTXOs and the transaction kernels are extremely compact. This has two important consequences: The blockchain a node needs to maintain is very small (on the order of a few gigabytes for a bitcoin-sized blockchain, and potentially optimizable to a few hundreds of megabytes). When a new node joins the network the amount of information that needs to be transferred is very small. In addition, the UTXO set cannot be tampered with. Adding or removing even one input or output would change the sum of the transactions to be something other than zero.","title":"Cut-through All The Way"},{"location":"technical/introduction-to-mimblewimble/#conclusion","text":"In this document we covered the basic principles that underlie a Mimblewimble blockchain. By using the addition properties of Elliptic Curve Cryptography, we're able to build transactions that are completely opaque but can still be properly validated. And by generalizing those properties to blocks, we can eliminate a large amount of blockchain data, allowing for great scaling and fast sync of new peers.","title":"Conclusion"},{"location":"technical/table-of-contents/","text":"Table of Contents This part of the documentation, explain more in details the technology behind Grin along with the building blocks used. Warning This documentation contains an high level overview of the Grin and Mimblewimble. For more details about the implentation please look at the Grin documentation on doc.rs . Introduction Introduction to Mimblewimble - A technical introduction to Mimblewimble Grin For Bitcoiners - Explaining Grin from a Bitcoiner's perspective Building Blocks This part covers some of the building blocks used in Grin. Merkle Mountain Ranges - Explain what are Merkle Mountain Ranges (MMRs) Merkle Proof - Explain what are Merkle Proof Switch Commitments - An introduction to Switch Commitments Blockchain and Consensus This parts details several mechanisms and structures used by Grin. Coinbase Maturity Rule - Explain the Coinbase maturity rule Fast Sync - Explain the fast sync process Proof-of-Work - An introduction to Cuckoo Cycle: Grin's Proof-of-Work Pruning Blockchain Data - Validation of a pruned MW blockchain State and Storage - Explain the state used in Grin Miscellaneous This section contains various documents about Grin and Mimblewimble. Contracts - Describe Grin smart contracts Dandelion - Describe the Dandelion protocol adapted to Grin Range Proof Format - Explanation of the byte output of a range proof in a Grin transaction","title":"Table of Contents"},{"location":"technical/table-of-contents/#table-of-contents","text":"This part of the documentation, explain more in details the technology behind Grin along with the building blocks used. Warning This documentation contains an high level overview of the Grin and Mimblewimble. For more details about the implentation please look at the Grin documentation on doc.rs .","title":"Table of Contents"},{"location":"technical/table-of-contents/#introduction","text":"Introduction to Mimblewimble - A technical introduction to Mimblewimble Grin For Bitcoiners - Explaining Grin from a Bitcoiner's perspective","title":"Introduction"},{"location":"technical/table-of-contents/#building-blocks","text":"This part covers some of the building blocks used in Grin. Merkle Mountain Ranges - Explain what are Merkle Mountain Ranges (MMRs) Merkle Proof - Explain what are Merkle Proof Switch Commitments - An introduction to Switch Commitments","title":"Building Blocks"},{"location":"technical/table-of-contents/#blockchain-and-consensus","text":"This parts details several mechanisms and structures used by Grin. Coinbase Maturity Rule - Explain the Coinbase maturity rule Fast Sync - Explain the fast sync process Proof-of-Work - An introduction to Cuckoo Cycle: Grin's Proof-of-Work Pruning Blockchain Data - Validation of a pruned MW blockchain State and Storage - Explain the state used in Grin","title":"Blockchain and Consensus"},{"location":"technical/table-of-contents/#miscellaneous","text":"This section contains various documents about Grin and Mimblewimble. Contracts - Describe Grin smart contracts Dandelion - Describe the Dandelion protocol adapted to Grin Range Proof Format - Explanation of the byte output of a range proof in a Grin transaction","title":"Miscellaneous"},{"location":"technical/blockchain-and-consensus/coinbase-maturity-rule/","text":"Coinbase Maturity Rule (aka Output Lock Heights) Coinbase outputs (block rewards & fees) are \"locked\" and require 1,440 confirmations (i.e 24 hours worth of blocks added to the chain) before they mature sufficiently to be spendable. This is to reduce the risk of later txs being reversed if a chain reorganization occurs. Bitcoin does something very similar, requiring 100 confirmations (Bitcoin blocks are every 10 minutes, Grin blocks are every 60 seconds) before mining rewards can be spent. Grin enforces coinbase maturity in both the transaction pool and the block validation pipeline. A transaction containing an input spending a coinbase output cannot be added to the transaction pool until it has sufficiently matured (based on current chain height and the height of the block producing the coinbase output). Similarly a block is invalid if it contains an input spending a coinbase output before it has sufficiently matured, based on the height of the block containing the input and the height of the block that originally produced the coinbase output. The maturity rule only applies to coinbase outputs, regular transaction outputs have an effective lock height of zero. An output consists of - features (currently coinbase vs. non-coinbase) commitment rG+vH rangeproof To spend a regular transaction output two conditions must be met. We need to show the output has not been previously spent and we need to prove ownership of the output. A Grin transaction consists of the following - A set of inputs, each referencing a previous output being spent. A set of new outputs that include - A value v and a blinding factor (private key) r multiplied on a curve and summed to be rG+vH A range proof that shows that v is non-negative. An explicit transaction fee in the clear. A signature, computed by taking the excess blinding value (the sum of all outputs plus the fee, minus the inputs) and using it as the private key. We can show the output is unspent by looking for the commitment in the current Output set. The Output set is authoritative; if the output exists in the Output set we know it has not yet been spent. If an output does not exist in the Output set we know it has either never existed, or that it previously existed and has been spent (we will not necessarily know which). To prove ownership we can verify the transaction signature. We can only have signed the transaction if the transaction sums to zero and we know both v and r . Knowing v and r we can uniquely identify the output (via its commitment) and we can prove ownership of the output by validating the signature on the original coinbase transaction. Grin does not permit duplicate commitments to exist in the Output set at the same time. But once an output is spent it is removed from the Output set and a duplicate commitment can be added back into the Output set. This is not necessarily recommended but Grin must handle this situation in a way that does not break consensus across the network. Several things complicate this situation - It is possible for two blocks to have identical rewards, particularly for the case of empty blocks, but also possible for non-empty blocks with transaction fees. It is possible for a non-coinbase output to have the same value as a coinbase output. It is possible (but not recommended) for a miner to reuse private keys. Grin does not allow duplicate commitments to exist in the Output set simultaneously. But the Output set is specific to the state of a particular chain fork. It is possible for duplicate identical commitments to exist simultaneously on different concurrent forks. And these duplicate commitments may have different \"lock heights\" at which they mature and become spendable on the different forks. Output O 1 from block B 1 spendable at height h 1 (on fork f 1 ) Output O 1 ' from block B 2 spendable at height h 2 (on fork f 2 ) The complication here is that input I 1 will spend either O 1 or O 1 ' depending on which fork the block containing I 1 exists on. And crucially I 1 may be valid at a particular block height on one fork but not the other. Said another way - a commitment may refer to multiple outputs, all of which may have different lock heights. And we must ensure we correctly identify which output is actually being spent and that the coinbase maturity rules are correctly enforced based on the current chain state. A coinbase output, locked with the coinbase maturity rule at a specific lock height, cannot be uniquely identified, and cannot be safely spent by their commitment alone. To spend a coinbase output we need to know one additional piece of information - The block the coinbase output originated from Given this, we can verify the height of the block and derive the \"lock height\" of the output (+ 1,000 blocks).","title":"Coinbase Maturity Rule"},{"location":"technical/blockchain-and-consensus/coinbase-maturity-rule/#coinbase-maturity-rule-aka-output-lock-heights","text":"Coinbase outputs (block rewards & fees) are \"locked\" and require 1,440 confirmations (i.e 24 hours worth of blocks added to the chain) before they mature sufficiently to be spendable. This is to reduce the risk of later txs being reversed if a chain reorganization occurs. Bitcoin does something very similar, requiring 100 confirmations (Bitcoin blocks are every 10 minutes, Grin blocks are every 60 seconds) before mining rewards can be spent. Grin enforces coinbase maturity in both the transaction pool and the block validation pipeline. A transaction containing an input spending a coinbase output cannot be added to the transaction pool until it has sufficiently matured (based on current chain height and the height of the block producing the coinbase output). Similarly a block is invalid if it contains an input spending a coinbase output before it has sufficiently matured, based on the height of the block containing the input and the height of the block that originally produced the coinbase output. The maturity rule only applies to coinbase outputs, regular transaction outputs have an effective lock height of zero. An output consists of - features (currently coinbase vs. non-coinbase) commitment rG+vH rangeproof To spend a regular transaction output two conditions must be met. We need to show the output has not been previously spent and we need to prove ownership of the output. A Grin transaction consists of the following - A set of inputs, each referencing a previous output being spent. A set of new outputs that include - A value v and a blinding factor (private key) r multiplied on a curve and summed to be rG+vH A range proof that shows that v is non-negative. An explicit transaction fee in the clear. A signature, computed by taking the excess blinding value (the sum of all outputs plus the fee, minus the inputs) and using it as the private key. We can show the output is unspent by looking for the commitment in the current Output set. The Output set is authoritative; if the output exists in the Output set we know it has not yet been spent. If an output does not exist in the Output set we know it has either never existed, or that it previously existed and has been spent (we will not necessarily know which). To prove ownership we can verify the transaction signature. We can only have signed the transaction if the transaction sums to zero and we know both v and r . Knowing v and r we can uniquely identify the output (via its commitment) and we can prove ownership of the output by validating the signature on the original coinbase transaction. Grin does not permit duplicate commitments to exist in the Output set at the same time. But once an output is spent it is removed from the Output set and a duplicate commitment can be added back into the Output set. This is not necessarily recommended but Grin must handle this situation in a way that does not break consensus across the network. Several things complicate this situation - It is possible for two blocks to have identical rewards, particularly for the case of empty blocks, but also possible for non-empty blocks with transaction fees. It is possible for a non-coinbase output to have the same value as a coinbase output. It is possible (but not recommended) for a miner to reuse private keys. Grin does not allow duplicate commitments to exist in the Output set simultaneously. But the Output set is specific to the state of a particular chain fork. It is possible for duplicate identical commitments to exist simultaneously on different concurrent forks. And these duplicate commitments may have different \"lock heights\" at which they mature and become spendable on the different forks. Output O 1 from block B 1 spendable at height h 1 (on fork f 1 ) Output O 1 ' from block B 2 spendable at height h 2 (on fork f 2 ) The complication here is that input I 1 will spend either O 1 or O 1 ' depending on which fork the block containing I 1 exists on. And crucially I 1 may be valid at a particular block height on one fork but not the other. Said another way - a commitment may refer to multiple outputs, all of which may have different lock heights. And we must ensure we correctly identify which output is actually being spent and that the coinbase maturity rules are correctly enforced based on the current chain state. A coinbase output, locked with the coinbase maturity rule at a specific lock height, cannot be uniquely identified, and cannot be safely spent by their commitment alone. To spend a coinbase output we need to know one additional piece of information - The block the coinbase output originated from Given this, we can verify the height of the block and derive the \"lock height\" of the output (+ 1,000 blocks).","title":"Coinbase Maturity Rule (aka Output Lock Heights)"},{"location":"technical/blockchain-and-consensus/fast-sync/","text":"Fast Sync In Grin, we call \"sync\" the process of synchronizing a new node or a node that hasn't been keeping up with the chain for a while, and bringing it up to the latest known most-worked block. Initial Block Download (or IBD) is often used by other blockchains, but this is problematic for Grin as it typically does not download full blocks. In short, a fast-sync in Grin does the following: Download all block headers, by chunks, on the most worked chain, as advertized by other nodes. Find a header sufficiently back from the chain head. This is called the node horizon as it's the furthest a node can reorganize its chain on a new fork if it were to occur without triggering another new full sync. Download the full state as it was at the horizon, including the unspent output, range proof and kernel data, as well as all corresponding MMRs. This is just one large zip file. Validate the full state. Download full blocks since the horizon to get to the chain head. In the rest of this section, we will elaborate on each of those steps.","title":"Fast Sync"},{"location":"technical/blockchain-and-consensus/fast-sync/#fast-sync","text":"In Grin, we call \"sync\" the process of synchronizing a new node or a node that hasn't been keeping up with the chain for a while, and bringing it up to the latest known most-worked block. Initial Block Download (or IBD) is often used by other blockchains, but this is problematic for Grin as it typically does not download full blocks. In short, a fast-sync in Grin does the following: Download all block headers, by chunks, on the most worked chain, as advertized by other nodes. Find a header sufficiently back from the chain head. This is called the node horizon as it's the furthest a node can reorganize its chain on a new fork if it were to occur without triggering another new full sync. Download the full state as it was at the horizon, including the unspent output, range proof and kernel data, as well as all corresponding MMRs. This is just one large zip file. Validate the full state. Download full blocks since the horizon to get to the chain head. In the rest of this section, we will elaborate on each of those steps.","title":"Fast Sync"},{"location":"technical/blockchain-and-consensus/proof-of-work/","text":"Grin's Proof-of-Work Warning Since this document was first published, Grin switched to a variant of Cuckoo cycle: Cuckatoo cycle. Alongside, a dual pow scheme was designed to ensure a fair initial coin distribution. You can read more about this here . This document is meant to outline, at a level suitable for someone without prior knowledge, the algorithms and processes currently involved in Grin's Proof-of-Work system. We'll start with a general overview of cycles in a graph and the Cuckoo Cycle algorithm which forms the basis of Grin's proof-of-work. We'll then move on to Grin-specific details, which will outline the other systems that combine with Cuckoo Cycle to form the entirety of mining in Grin. Please note that Grin is currently under active development, and any and all of this is subject to (and will) change before a general release. Graphs and Cuckoo Cycle Grin's basic Proof-of-Work algorithm is called Cuckoo Cycle, which is specifically designed to be resistant to Bitcoin style hardware arms-races. It is primarily a memory bound algorithm, which, (at least in theory,) means that solution time is bound by memory bandwidth rather than raw processor or GPU speed. As such, mining Cuckoo Cycle solutions should be viable on most commodity hardware, and require far less energy than most other GPU, CPU or ASIC-bound proof of work algorithms. The Cuckoo Cycle POW is the work of John Tromp, and the most up-to-date documentation and implementations can be found in his github repository . The white paper is the best source of further technical details. There is also a podcast with Mike from Monero Monitor in which John Tromp talks at length about Cuckoo Cycle; recommended listening for anyone wanting more background on Cuckoo Cycle, including more technical detail, the history of the algorithm's development and some of the motivations behind it. Cycles in a Graph Cuckoo Cycle is an algorithm meant to detect cycles in a bipartite graph of N nodes and M edges. In plain terms, a bipartite graph is one in which edges (i.e. lines connecting nodes) travel only between 2 separate groups of nodes. In the case of the Cuckoo hashtable in Cuckoo Cycle, one side of the graph is an array numbered with odd indices (up to the size of the graph), and the other is numbered with even indices. A node is simply a numbered 'space' on either side of the Cuckoo Table, and an Edge is a line connecting two nodes on opposite sides. The simple graph below denotes just such a graph, with 4 nodes on the 'even' side (top), 4 nodes on the odd side (bottom) and zero Edges (i.e. no lines connecting any nodes.) A graph of 8 Nodes with Zero Edges Let's throw a few Edges into the graph now, randomly: 8 Nodes with 4 Edges, no solution We now have a randomly-generated graph with 8 nodes (N) and 4 edges (M), or an NxM graph where N=8 and M=4. Our basic Proof-of-Work is now concerned with finding 'cycles' of a certain length within this random graph, or, put simply, a series of connected nodes starting and ending on the same node. So, if we were looking for a cycle of length 4 (a path connecting 4 nodes, starting and ending on the same node), one cannot be detected in this graph. Adjusting the number of Edges M relative to the number of Nodes N changes the difficulty of the cycle-finding problem, and the probability that a cycle exists in the current graph. For instance, if our POW problem were concerned with finding a cycle of length 4 in the graph, the current difficulty of 4/8 (M/N) would mean that all 4 edges would need to be randomly generated in a perfect cycle (from 0-5-4-1-0) in order for there to be a solution. Let's add a few more edges, again at random: 8 Nodes with 7 Edges Where we can find a cycle: Cycle Found from 0-5-4-1-0 If you increase the number of edges relative to the number of nodes, you increase the probability that a solution exists. With a few more edges added to the graph above, a cycle of length 4 has appeared from 0-5-4-1-0, and the graph has a solution. Thus, modifying the ratio M/N changes the number of expected occurrences of a cycle for a graph with randomly generated edges. For a small graph such as the one above, determining whether a cycle of a certain length exists is trivial. But as the graphs get larger, detecting such cycles becomes more difficult. For instance, does this graph have a cycle of length 8, i.e. 8 connected nodes starting and ending on the same node? Meat-space Cycle Detection exercise The answer is left as an exercise to the reader, but the overall takeaways are: Detecting cycles in a graph becomes more difficult exercise as the size of a graph grows. The probability of a cycle of a given length in a graph increases as M/N becomes larger, i.e. you add more edges relative to the number of nodes in a graph. Cuckoo Cycle The Cuckoo Cycle algorithm is a specialized algorithm designed to solve exactly this problem, and it does so by inserting values into a structure called a 'Cuckoo Hashtable' according to a hash which maps nodes into possible locations in two separate arrays. This document won't go into detail on the base algorithm, as it's outlined plainly enough in section 5 of the white paper . There are also several variants on the algorithm that make various speed/memory tradeoffs, again beyond the scope of this document. However, there are a few details following from the above that we need to keep in mind before going on to more technical aspects of Grin's proof-of-work. The 'random' edges in the graph demonstrated above are not actually random but are generated by putting edge indices (0..N) through a seeded hash function, SIPHASH. Each edge index is put through the SIPHASH function twice to create two edge endpoints, with the first input value being 2 * edge_index, and the second 2 * edge_index+1. The seed for this function is based on a hash of a block header, outlined further below. The 'Proof' created by this algorithm is a set of nonces that generate a cycle of length 42, which can be trivially validated by other peers. Two main parameters, as explained above, are passed into the Cuckoo Cycle algorithm that affect the probability of a solution, and the time it takes to search the graph for a solution: The M/N ratio outlined above, which controls the number of edges relative to the size of the graph. Cuckoo Cycle fixes M at N/2, which limits the number of cycles to a few at most. The size of the graph itself How these parameters interact in practice is looked at in more detail below . Now, (hopefully) armed with a basic understanding of what the Cuckoo Cycle algorithm is intended to do, as well as the parameters that affect how difficult it is to find a solution, we move on to the other portions of Grin's POW system. Mining in Grin The Cuckoo Cycle outlined above forms the basis of Grin's mining process, however Grin uses Cuckoo Cycle in tandem with several other systems to create a Proof-of-Work. Additional Difficulty Control In order to provide additional difficulty control in a manner that meets the needs of a network with constantly evolving hashpower availability, a further Hashcash-based difficulty check is applied to potential solution sets as follows: If the Blake2b hash of a potential set of solution nonces (currently an array of 42 u32s representing the cycle nonces,) is less than an evolving difficulty target T, then the solution is considered valid. More precisely, the proof difficulty is calculated as the maximum target hash (2^256) divided by the current hash, rounded to give an integer. If this integer is larger than the evolving network difficulty, the POW is considered valid and the block is submit to the chain for validation. In other words, a potential proof, as well as containing a valid Cuckoo Cycle, also needs to hash to a value higher than the target difficulty. This difficulty is derived from: Evolving Network Difficulty The difficulty target is intended to evolve according to the available network hashpower, with the goal of keeping the average block solution time within range of a target (currently 60 seconds, though this is subject to change). The difficulty calculation is based on both Digishield and GravityWave family of difficulty computation, coming to something very close to ZCash. The reference difficulty is an average of the difficulty over a window of 23 blocks (the current consensus value). The corresponding timespan is calculated by using the difference between the median timestamps at the beginning and the end of the window. If the timespan is higher or lower than a certain range, (adjusted with a dampening factor to allow for normal variation,) then the difficulty is raised or lowered to a value aiming for the target block solve time. The Mining Loop All of these systems are put together in the mining loop, which attempts to create valid Proofs-of-Work to create the latest block in the chain. The following is an outline of what the main mining loop does during a single iteration: Get the latest chain state and build a block on top of it, which includes A Block Header with new values particular to this mining attempt, which are: The latest target difficulty as selected by the evolving network difficulty algorithm A set of transactions available for validation selected from the transaction pool A coinbase transaction (which we're hoping to give to ourselves) The current timestamp A randomly generated nonce to add further randomness to the header's hash The merkle root of the UTXO set and fees (not yet implemented) Then, a sub-loop runs for a set amount of time, currently configured at 2 seconds, where the following happens: The new block header is hashed to create a hash value The cuckoo graph generator is initialized, which accepts as parameters: The hash of the potential block header, which is to be used as the key to a SIPHASH function that will generate pairs of locations for each element in a set of nonces 0..N in the graph. The size of the graph (a consensus value). An easiness value, (a consensus value) representing the M/N ratio described above denoting the probability of a solution appearing in the graph The Cuckoo Cycle detection algorithm tries to find a solution (i.e. a cycle of length 42) within the generated graph. If a cycle is found, a Blake2b hash of the proof is created and is compared to the current target difficulty, as outlined in Additional Difficulty Control above. If the Blake2b Hash difficulty is greater than or equal to the target difficulty, the block is sent to the transaction pool, propagated amongst peers for validation, and work begins on the next block. If the Blake2b Hash difficulty is less than the target difficulty, the proof is thrown out and the timed loop continues. If no solution is found, increment the nonce in the header by 1, and update the header's timestamp so the next iteration hashes a different value for seeding the next loop's graph generation step. If the loop times out with no solution found, start over again from the top, collecting new transactions and creating a new block altogether. Mining Loop Difficulty Control and Timing Controlling the overall difficulty of the mining loop requires finding a balance between the three values outlined above: Graph size (currently represented as a bit-shift value n representing a size of 2^n nodes, consensus value DEFAULT_SIZESHIFT). Smaller graphs can be exhaustively searched more quickly, but will also have fewer solutions for a given easiness value. A very small graph needs a higher easiness value to have the same chance to have a solution as a larger graph with a lower easiness value. The 'Easiness' consensus value, or the M/N ratio of the graph expressed as a percentage. The higher this value, the more likely it is a generated graph will contain a solution. In tandem with the above, the larger the graph, the more solutions it will contain for a given easiness value. The Cuckoo Cycle implementations fix this M to N/2, giving a ratio of 50% The evolving network difficulty hash. These values need to be carefully tweaked in order for the mining algorithm to find the right balance between the cuckoo graph size and the evolving difficulty. The POW needs to remain mostly Cuckoo Cycle based, but still allow for reasonably short block times that allow new transactions to be quickly processed. If the graph size is too low and the easiness too high, for instance, then many cuckoo cycle solutions can easily be found for a given block, and the POW will start to favour those who can hash faster, precisely what Cuckoo Cycle is trying to avoid. If the graph is too large and easiness too low, however, then it can potentially take any solver a long time to find a solution in a single graph, well outside a window in which you'd like to stop to collect new transactions. These values are currently set to 2^12 for the graph size and 50% (as fixed by Cuckoo Cycle) for the easiness value, however the size is only a temporary values for testing. The current miner implementation is very unoptimized, and the graph size will need to be changed as faster and more optimized Cuckoo Cycle algorithms are put in place. Pooling Capability Contrary to some existing concerns about Cuckoo Cycle's poolability, the POW implementation in Grin as described above is perfectly suited to a mining pool. While it may be difficult to prove efforts to solve a single graph in isolation, the combination of factors within Grin's proof-of-work combine to enforce a notion called 'progress-freeness', which enables 'poolability' as well as a level of fairness among all miners. Progress Freeness Progress-freeness is central to the 'poolability' of a proof-of-work, and is simply based on the idea that a solution to a POW problem can be found within a reasonable amount of time. For instance, if a blockchain has a one minute POW time and miners have to spend one minute on average to find a solution, this still satisfies the POW requirement but gives a strong advantage to big miners. In such a setup, small miners will generally lose at least one minute every time while larger miners can move on as soon as they find a solution. So in order to keep mining relatively progress-free, a POW that requires multiple solution attempts with each attempt taking a relatively small amount of time is desirable. Following from this, Grin's progress-freeness is due to the fact that a solution to a Cuckoo with Grin's default parameters can typically be found in under a second on most GPUs, and there is the additional requirement of the Blake2b difficulty check on top of that. Members of a pool are thus able to prove they're working on a solution to a block by submitting valid Cuckoo solutions (or a small bundle of them) that simply fall under the current network target difficulty.","title":"Proof-of-Work"},{"location":"technical/blockchain-and-consensus/proof-of-work/#grins-proof-of-work","text":"Warning Since this document was first published, Grin switched to a variant of Cuckoo cycle: Cuckatoo cycle. Alongside, a dual pow scheme was designed to ensure a fair initial coin distribution. You can read more about this here . This document is meant to outline, at a level suitable for someone without prior knowledge, the algorithms and processes currently involved in Grin's Proof-of-Work system. We'll start with a general overview of cycles in a graph and the Cuckoo Cycle algorithm which forms the basis of Grin's proof-of-work. We'll then move on to Grin-specific details, which will outline the other systems that combine with Cuckoo Cycle to form the entirety of mining in Grin. Please note that Grin is currently under active development, and any and all of this is subject to (and will) change before a general release.","title":"Grin's Proof-of-Work"},{"location":"technical/blockchain-and-consensus/proof-of-work/#graphs-and-cuckoo-cycle","text":"Grin's basic Proof-of-Work algorithm is called Cuckoo Cycle, which is specifically designed to be resistant to Bitcoin style hardware arms-races. It is primarily a memory bound algorithm, which, (at least in theory,) means that solution time is bound by memory bandwidth rather than raw processor or GPU speed. As such, mining Cuckoo Cycle solutions should be viable on most commodity hardware, and require far less energy than most other GPU, CPU or ASIC-bound proof of work algorithms. The Cuckoo Cycle POW is the work of John Tromp, and the most up-to-date documentation and implementations can be found in his github repository . The white paper is the best source of further technical details. There is also a podcast with Mike from Monero Monitor in which John Tromp talks at length about Cuckoo Cycle; recommended listening for anyone wanting more background on Cuckoo Cycle, including more technical detail, the history of the algorithm's development and some of the motivations behind it.","title":"Graphs and Cuckoo Cycle"},{"location":"technical/blockchain-and-consensus/proof-of-work/#cycles-in-a-graph","text":"Cuckoo Cycle is an algorithm meant to detect cycles in a bipartite graph of N nodes and M edges. In plain terms, a bipartite graph is one in which edges (i.e. lines connecting nodes) travel only between 2 separate groups of nodes. In the case of the Cuckoo hashtable in Cuckoo Cycle, one side of the graph is an array numbered with odd indices (up to the size of the graph), and the other is numbered with even indices. A node is simply a numbered 'space' on either side of the Cuckoo Table, and an Edge is a line connecting two nodes on opposite sides. The simple graph below denotes just such a graph, with 4 nodes on the 'even' side (top), 4 nodes on the odd side (bottom) and zero Edges (i.e. no lines connecting any nodes.) A graph of 8 Nodes with Zero Edges Let's throw a few Edges into the graph now, randomly: 8 Nodes with 4 Edges, no solution We now have a randomly-generated graph with 8 nodes (N) and 4 edges (M), or an NxM graph where N=8 and M=4. Our basic Proof-of-Work is now concerned with finding 'cycles' of a certain length within this random graph, or, put simply, a series of connected nodes starting and ending on the same node. So, if we were looking for a cycle of length 4 (a path connecting 4 nodes, starting and ending on the same node), one cannot be detected in this graph. Adjusting the number of Edges M relative to the number of Nodes N changes the difficulty of the cycle-finding problem, and the probability that a cycle exists in the current graph. For instance, if our POW problem were concerned with finding a cycle of length 4 in the graph, the current difficulty of 4/8 (M/N) would mean that all 4 edges would need to be randomly generated in a perfect cycle (from 0-5-4-1-0) in order for there to be a solution. Let's add a few more edges, again at random: 8 Nodes with 7 Edges Where we can find a cycle: Cycle Found from 0-5-4-1-0 If you increase the number of edges relative to the number of nodes, you increase the probability that a solution exists. With a few more edges added to the graph above, a cycle of length 4 has appeared from 0-5-4-1-0, and the graph has a solution. Thus, modifying the ratio M/N changes the number of expected occurrences of a cycle for a graph with randomly generated edges. For a small graph such as the one above, determining whether a cycle of a certain length exists is trivial. But as the graphs get larger, detecting such cycles becomes more difficult. For instance, does this graph have a cycle of length 8, i.e. 8 connected nodes starting and ending on the same node? Meat-space Cycle Detection exercise The answer is left as an exercise to the reader, but the overall takeaways are: Detecting cycles in a graph becomes more difficult exercise as the size of a graph grows. The probability of a cycle of a given length in a graph increases as M/N becomes larger, i.e. you add more edges relative to the number of nodes in a graph.","title":"Cycles in a Graph"},{"location":"technical/blockchain-and-consensus/proof-of-work/#cuckoo-cycle","text":"The Cuckoo Cycle algorithm is a specialized algorithm designed to solve exactly this problem, and it does so by inserting values into a structure called a 'Cuckoo Hashtable' according to a hash which maps nodes into possible locations in two separate arrays. This document won't go into detail on the base algorithm, as it's outlined plainly enough in section 5 of the white paper . There are also several variants on the algorithm that make various speed/memory tradeoffs, again beyond the scope of this document. However, there are a few details following from the above that we need to keep in mind before going on to more technical aspects of Grin's proof-of-work. The 'random' edges in the graph demonstrated above are not actually random but are generated by putting edge indices (0..N) through a seeded hash function, SIPHASH. Each edge index is put through the SIPHASH function twice to create two edge endpoints, with the first input value being 2 * edge_index, and the second 2 * edge_index+1. The seed for this function is based on a hash of a block header, outlined further below. The 'Proof' created by this algorithm is a set of nonces that generate a cycle of length 42, which can be trivially validated by other peers. Two main parameters, as explained above, are passed into the Cuckoo Cycle algorithm that affect the probability of a solution, and the time it takes to search the graph for a solution: The M/N ratio outlined above, which controls the number of edges relative to the size of the graph. Cuckoo Cycle fixes M at N/2, which limits the number of cycles to a few at most. The size of the graph itself How these parameters interact in practice is looked at in more detail below . Now, (hopefully) armed with a basic understanding of what the Cuckoo Cycle algorithm is intended to do, as well as the parameters that affect how difficult it is to find a solution, we move on to the other portions of Grin's POW system.","title":"Cuckoo Cycle"},{"location":"technical/blockchain-and-consensus/proof-of-work/#mining-in-grin","text":"The Cuckoo Cycle outlined above forms the basis of Grin's mining process, however Grin uses Cuckoo Cycle in tandem with several other systems to create a Proof-of-Work.","title":"Mining in Grin"},{"location":"technical/blockchain-and-consensus/proof-of-work/#additional-difficulty-control","text":"In order to provide additional difficulty control in a manner that meets the needs of a network with constantly evolving hashpower availability, a further Hashcash-based difficulty check is applied to potential solution sets as follows: If the Blake2b hash of a potential set of solution nonces (currently an array of 42 u32s representing the cycle nonces,) is less than an evolving difficulty target T, then the solution is considered valid. More precisely, the proof difficulty is calculated as the maximum target hash (2^256) divided by the current hash, rounded to give an integer. If this integer is larger than the evolving network difficulty, the POW is considered valid and the block is submit to the chain for validation. In other words, a potential proof, as well as containing a valid Cuckoo Cycle, also needs to hash to a value higher than the target difficulty. This difficulty is derived from:","title":"Additional Difficulty Control"},{"location":"technical/blockchain-and-consensus/proof-of-work/#evolving-network-difficulty","text":"The difficulty target is intended to evolve according to the available network hashpower, with the goal of keeping the average block solution time within range of a target (currently 60 seconds, though this is subject to change). The difficulty calculation is based on both Digishield and GravityWave family of difficulty computation, coming to something very close to ZCash. The reference difficulty is an average of the difficulty over a window of 23 blocks (the current consensus value). The corresponding timespan is calculated by using the difference between the median timestamps at the beginning and the end of the window. If the timespan is higher or lower than a certain range, (adjusted with a dampening factor to allow for normal variation,) then the difficulty is raised or lowered to a value aiming for the target block solve time.","title":"Evolving Network Difficulty"},{"location":"technical/blockchain-and-consensus/proof-of-work/#the-mining-loop","text":"All of these systems are put together in the mining loop, which attempts to create valid Proofs-of-Work to create the latest block in the chain. The following is an outline of what the main mining loop does during a single iteration: Get the latest chain state and build a block on top of it, which includes A Block Header with new values particular to this mining attempt, which are: The latest target difficulty as selected by the evolving network difficulty algorithm A set of transactions available for validation selected from the transaction pool A coinbase transaction (which we're hoping to give to ourselves) The current timestamp A randomly generated nonce to add further randomness to the header's hash The merkle root of the UTXO set and fees (not yet implemented) Then, a sub-loop runs for a set amount of time, currently configured at 2 seconds, where the following happens: The new block header is hashed to create a hash value The cuckoo graph generator is initialized, which accepts as parameters: The hash of the potential block header, which is to be used as the key to a SIPHASH function that will generate pairs of locations for each element in a set of nonces 0..N in the graph. The size of the graph (a consensus value). An easiness value, (a consensus value) representing the M/N ratio described above denoting the probability of a solution appearing in the graph The Cuckoo Cycle detection algorithm tries to find a solution (i.e. a cycle of length 42) within the generated graph. If a cycle is found, a Blake2b hash of the proof is created and is compared to the current target difficulty, as outlined in Additional Difficulty Control above. If the Blake2b Hash difficulty is greater than or equal to the target difficulty, the block is sent to the transaction pool, propagated amongst peers for validation, and work begins on the next block. If the Blake2b Hash difficulty is less than the target difficulty, the proof is thrown out and the timed loop continues. If no solution is found, increment the nonce in the header by 1, and update the header's timestamp so the next iteration hashes a different value for seeding the next loop's graph generation step. If the loop times out with no solution found, start over again from the top, collecting new transactions and creating a new block altogether.","title":"The Mining Loop"},{"location":"technical/blockchain-and-consensus/proof-of-work/#mining-loop-difficulty-control-and-timing","text":"Controlling the overall difficulty of the mining loop requires finding a balance between the three values outlined above: Graph size (currently represented as a bit-shift value n representing a size of 2^n nodes, consensus value DEFAULT_SIZESHIFT). Smaller graphs can be exhaustively searched more quickly, but will also have fewer solutions for a given easiness value. A very small graph needs a higher easiness value to have the same chance to have a solution as a larger graph with a lower easiness value. The 'Easiness' consensus value, or the M/N ratio of the graph expressed as a percentage. The higher this value, the more likely it is a generated graph will contain a solution. In tandem with the above, the larger the graph, the more solutions it will contain for a given easiness value. The Cuckoo Cycle implementations fix this M to N/2, giving a ratio of 50% The evolving network difficulty hash. These values need to be carefully tweaked in order for the mining algorithm to find the right balance between the cuckoo graph size and the evolving difficulty. The POW needs to remain mostly Cuckoo Cycle based, but still allow for reasonably short block times that allow new transactions to be quickly processed. If the graph size is too low and the easiness too high, for instance, then many cuckoo cycle solutions can easily be found for a given block, and the POW will start to favour those who can hash faster, precisely what Cuckoo Cycle is trying to avoid. If the graph is too large and easiness too low, however, then it can potentially take any solver a long time to find a solution in a single graph, well outside a window in which you'd like to stop to collect new transactions. These values are currently set to 2^12 for the graph size and 50% (as fixed by Cuckoo Cycle) for the easiness value, however the size is only a temporary values for testing. The current miner implementation is very unoptimized, and the graph size will need to be changed as faster and more optimized Cuckoo Cycle algorithms are put in place.","title":"Mining Loop Difficulty Control and Timing"},{"location":"technical/blockchain-and-consensus/proof-of-work/#pooling-capability","text":"Contrary to some existing concerns about Cuckoo Cycle's poolability, the POW implementation in Grin as described above is perfectly suited to a mining pool. While it may be difficult to prove efforts to solve a single graph in isolation, the combination of factors within Grin's proof-of-work combine to enforce a notion called 'progress-freeness', which enables 'poolability' as well as a level of fairness among all miners.","title":"Pooling Capability"},{"location":"technical/blockchain-and-consensus/proof-of-work/#progress-freeness","text":"Progress-freeness is central to the 'poolability' of a proof-of-work, and is simply based on the idea that a solution to a POW problem can be found within a reasonable amount of time. For instance, if a blockchain has a one minute POW time and miners have to spend one minute on average to find a solution, this still satisfies the POW requirement but gives a strong advantage to big miners. In such a setup, small miners will generally lose at least one minute every time while larger miners can move on as soon as they find a solution. So in order to keep mining relatively progress-free, a POW that requires multiple solution attempts with each attempt taking a relatively small amount of time is desirable. Following from this, Grin's progress-freeness is due to the fact that a solution to a Cuckoo with Grin's default parameters can typically be found in under a second on most GPUs, and there is the additional requirement of the Blake2b difficulty check on top of that. Members of a pool are thus able to prove they're working on a solution to a block by submitting valid Cuckoo solutions (or a small bundle of them) that simply fall under the current network target difficulty.","title":"Progress Freeness"},{"location":"technical/blockchain-and-consensus/pruning-blockchain-data/","text":"Pruning Blockchain Data One of the principal attractions of Mimblewimble is its theoretical space efficiency. Indeed, a trusted or pre-validated full blockchain state only requires unspent transaction outputs, which could be tiny. The grin blockchain includes the following types of data (we assume prior understanding of the Mimblewimble protocol): Transaction outputs, which include for each output: A Pedersen commitment (33 bytes). A range proof (over 5KB at this time). Transaction inputs, which are just output references (32 bytes). Transaction \"proofs\", which include for each transaction: The excess commitment sum for the transaction (33 bytes). A signature generated with the excess (71 bytes average). A block header includes Merkle trees and proof of work (about 250 bytes). Assuming a blockchain of a million blocks, 10 million transactions (2 inputs, 2.5 outputs average) and 100,000 unspent outputs, we get the following approximate sizes with a full chain (no pruning, no cut-through): 128GB of transaction data (inputs and outputs). 1 GB of transaction proof data. 250MB of block headers. Total chain size around 130GB. Total chain size, after cut-through (but incl. headers) of 1.8GB. UTXO size of 520MB. Total chain size, without range proofs of 4GB. UTXO size, without range proofs of 3.3MB. We note that out of all that data, once the chain has been fully validated, only the set of UTXO commitments is strictly required for a node to function. There may be several contexts in which data can be pruned: A fully validating node may get rid of some data it has already validated to free space. A partially validating node (similar to SPV) may not be interested in either receiving or keeping all the data. When a new node joins the network, it may temporarily behave as a partially validating node to make it available for use faster, even if it ultimately becomes a fully validating node. Validation of Fully Pruned State Pruning needs to remove as much data as possible while keeping all the guarantees of a full Mimblewimble-style validation. This is necessary to keep a pruning node state's sane, but also on first fast sync, where only the minimum amount of data is sent to a new node. The full validation of the chain state requires that: All kernel signatures verify against their public keys. The sum of all UTXO commitments, minus the supply is a valid public key (can be used to sign the empty string). The sum of all kernel pubkeys equals the sum of all UTXO commitments, minus the supply. The root hashes of the UTXO PMMR, the range proofs PMMR and the kernels MMR match a block header with a valid Proof of Work chain. All range proofs are valid. In addition, while not necessary to validate the full chain state, to be able to accept and validate new blocks additional data is required: The output features, making the full output data necessary for all UTXOs. At minimum, this requires the following data: The block headers chain. All kernels, in order of inclusion in the chain. This also allows the reconstruction of the kernel MMR. All unspent outputs. The UTXO MMR and the range proof MMR (to learn the hashes of pruned data). Note that further pruning could be obtained by requiring the validation of only a subset of the range proofs, chosen randomly by the validating node.","title":"Pruning Blockchain Data"},{"location":"technical/blockchain-and-consensus/pruning-blockchain-data/#pruning-blockchain-data","text":"One of the principal attractions of Mimblewimble is its theoretical space efficiency. Indeed, a trusted or pre-validated full blockchain state only requires unspent transaction outputs, which could be tiny. The grin blockchain includes the following types of data (we assume prior understanding of the Mimblewimble protocol): Transaction outputs, which include for each output: A Pedersen commitment (33 bytes). A range proof (over 5KB at this time). Transaction inputs, which are just output references (32 bytes). Transaction \"proofs\", which include for each transaction: The excess commitment sum for the transaction (33 bytes). A signature generated with the excess (71 bytes average). A block header includes Merkle trees and proof of work (about 250 bytes). Assuming a blockchain of a million blocks, 10 million transactions (2 inputs, 2.5 outputs average) and 100,000 unspent outputs, we get the following approximate sizes with a full chain (no pruning, no cut-through): 128GB of transaction data (inputs and outputs). 1 GB of transaction proof data. 250MB of block headers. Total chain size around 130GB. Total chain size, after cut-through (but incl. headers) of 1.8GB. UTXO size of 520MB. Total chain size, without range proofs of 4GB. UTXO size, without range proofs of 3.3MB. We note that out of all that data, once the chain has been fully validated, only the set of UTXO commitments is strictly required for a node to function. There may be several contexts in which data can be pruned: A fully validating node may get rid of some data it has already validated to free space. A partially validating node (similar to SPV) may not be interested in either receiving or keeping all the data. When a new node joins the network, it may temporarily behave as a partially validating node to make it available for use faster, even if it ultimately becomes a fully validating node.","title":"Pruning Blockchain Data"},{"location":"technical/blockchain-and-consensus/pruning-blockchain-data/#validation-of-fully-pruned-state","text":"Pruning needs to remove as much data as possible while keeping all the guarantees of a full Mimblewimble-style validation. This is necessary to keep a pruning node state's sane, but also on first fast sync, where only the minimum amount of data is sent to a new node. The full validation of the chain state requires that: All kernel signatures verify against their public keys. The sum of all UTXO commitments, minus the supply is a valid public key (can be used to sign the empty string). The sum of all kernel pubkeys equals the sum of all UTXO commitments, minus the supply. The root hashes of the UTXO PMMR, the range proofs PMMR and the kernels MMR match a block header with a valid Proof of Work chain. All range proofs are valid. In addition, while not necessary to validate the full chain state, to be able to accept and validate new blocks additional data is required: The output features, making the full output data necessary for all UTXOs. At minimum, this requires the following data: The block headers chain. All kernels, in order of inclusion in the chain. This also allows the reconstruction of the kernel MMR. All unspent outputs. The UTXO MMR and the range proof MMR (to learn the hashes of pruned data). Note that further pruning could be obtained by requiring the validation of only a subset of the range proofs, chosen randomly by the validating node.","title":"Validation of Fully Pruned State"},{"location":"technical/blockchain-and-consensus/state-and-storage/","text":"State and Storage The Grin State Structure The full state of a Grin chain consists of all the following data: The full unspent output (UTXO) set. The range proof for each output. All the transaction kernels. A MMR for each of the above (with the exception that the output MMR includes hashes for all outputs, not only the unspent ones). In addition, all headers in the chain are required to anchor the above state with a valid proof of work (the state corresponds to the most worked chain). We note that once each range proof is validated and the sum of all kernels commitment is computed, range proofs and kernels are not strictly necessary for a node to function anymore. Validation With a full Grin state, we can validate the following: The kernel signature is valid against its commitment (public key). This proves the kernel is valid. The sum of all kernel commitments equals the sum of all UTXO commitments minus the total supply. This proves that kernels and output commitments are all valid and no coins have unexpectedly been created. All UTXOs, range proofs and kernels hashes are present in their respective MMR and those MMRs hash to a valid root. A known block header with the most work at a given point in time includes the roots of the 3 MMRs. This validates the MMRs and proves that the whole state has been produced by the most worked chain. MMRs and Pruning The data used to produce the hashes for leaf nodes in each MMR (in addition to their position is the following: The output MMR hashes the feature field and the commitments of all outputs since genesis. The range proof MMR hashes the whole range proof data. The kernel MMR hashes all fields of the kernel: feature, fee, lock height, excess commitment and excess signature. Note that all outputs, range proofs and kernels are added in their respective MMRs in the order they occur in each block (recall that block data is required to be sorted). As outputs get spent, both their commitment and range proof data can be removed. In addition, the corresponding output and range proof MMRs can be pruned. State Storage Data storage for outputs, range proofs and kernels in Grin is simple: a plain append-only file that's memory-mapped for data access. As outputs get spent, a remove log maintains which positions can be removed. Those positions nicely match MMR node positions as they're all inserted in the same order. When the remove log gets large, corresponding files can be occasionally compacted by rewriting them without the removed pieces (also append-only) and the remove log can be emptied. As for MMRs, we need to add a little more complexity.","title":"State and Storage"},{"location":"technical/blockchain-and-consensus/state-and-storage/#state-and-storage","text":"","title":"State and Storage"},{"location":"technical/blockchain-and-consensus/state-and-storage/#the-grin-state","text":"","title":"The Grin State"},{"location":"technical/blockchain-and-consensus/state-and-storage/#structure","text":"The full state of a Grin chain consists of all the following data: The full unspent output (UTXO) set. The range proof for each output. All the transaction kernels. A MMR for each of the above (with the exception that the output MMR includes hashes for all outputs, not only the unspent ones). In addition, all headers in the chain are required to anchor the above state with a valid proof of work (the state corresponds to the most worked chain). We note that once each range proof is validated and the sum of all kernels commitment is computed, range proofs and kernels are not strictly necessary for a node to function anymore.","title":"Structure"},{"location":"technical/blockchain-and-consensus/state-and-storage/#validation","text":"With a full Grin state, we can validate the following: The kernel signature is valid against its commitment (public key). This proves the kernel is valid. The sum of all kernel commitments equals the sum of all UTXO commitments minus the total supply. This proves that kernels and output commitments are all valid and no coins have unexpectedly been created. All UTXOs, range proofs and kernels hashes are present in their respective MMR and those MMRs hash to a valid root. A known block header with the most work at a given point in time includes the roots of the 3 MMRs. This validates the MMRs and proves that the whole state has been produced by the most worked chain.","title":"Validation"},{"location":"technical/blockchain-and-consensus/state-and-storage/#mmrs-and-pruning","text":"The data used to produce the hashes for leaf nodes in each MMR (in addition to their position is the following: The output MMR hashes the feature field and the commitments of all outputs since genesis. The range proof MMR hashes the whole range proof data. The kernel MMR hashes all fields of the kernel: feature, fee, lock height, excess commitment and excess signature. Note that all outputs, range proofs and kernels are added in their respective MMRs in the order they occur in each block (recall that block data is required to be sorted). As outputs get spent, both their commitment and range proof data can be removed. In addition, the corresponding output and range proof MMRs can be pruned.","title":"MMRs and Pruning"},{"location":"technical/blockchain-and-consensus/state-and-storage/#state-storage","text":"Data storage for outputs, range proofs and kernels in Grin is simple: a plain append-only file that's memory-mapped for data access. As outputs get spent, a remove log maintains which positions can be removed. Those positions nicely match MMR node positions as they're all inserted in the same order. When the remove log gets large, corresponding files can be occasionally compacted by rewriting them without the removed pieces (also append-only) and the remove log can be emptied. As for MMRs, we need to add a little more complexity.","title":"State Storage"},{"location":"technical/building-blocks/merkle-mountain-ranges/","text":"Merkle Mountain Ranges Structure Merkle Mountain Ranges[1] are an alternative to Merkle trees[2]. While the latter relies on perfectly balanced binary trees, the former can be seen either as list of perfectly balance binary trees or a single binary tree that would have been truncated from the top right. A Merkle Mountain Range (MMR) is strictly append-only: elements are added from the left to the right, adding a parent as soon as 2 children exist, filling up the range accordingly. This illustrates a range with 11 inserted leaves and total size 19, where each node is annotated with its order of insertion. Height 3 14 / \\ / \\ / \\ / \\ 2 6 13 / \\ / \\ 1 2 5 9 12 17 / \\ / \\ / \\ / \\ / \\ 0 0 1 3 4 7 8 10 11 15 16 18 This can be represented as a flat list, here storing the height of each node at their position of insertion: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 0 0 1 0 0 1 2 0 0 1 0 0 1 2 3 0 0 1 0 This structure can be fully described simply from its size (19). It's also fairly simple, using fast binary operations, to navigate within a MMR. Given a node's position n , we can compute its height, the position of its parent, its siblings, etc. Hashing and Bagging Just like with Merkle trees, parent nodes in a MMR have for value the hash of their 2 children. Grin uses the Blake2b hash function throughout, and always prepends the node's position in the MMR before hashing to avoid collisions. So for a leaf l at index n storing data D (in the case of an output, the data is its Pedersen commitment, for example), we have: Node(l) = Blake2b(n | D) And for any parent p at index m : Node(p) = Blake2b(m | Node(left_child(p)) | Node(right_child(p))) Contrarily to a Merkle tree, a MMR generally has no single root by construction so we need a method to compute one (otherwise it would defeat the purpose of using a hash tree). This process is called \"bagging the peaks\" for reasons described in [1]. First, we identify the peaks of the MMR; we will define one method of doing so here. We first write another small example MMR but with the indexes written as binary (instead of decimal), starting from 1: Height 2 111 / \\ 1 11 110 1010 / \\ / \\ / \\ 0 1 10 100 101 1000 1001 1011 This MMR has 11 nodes and its peaks are at position 111 (7), 1010 (10) and 1011 (11). We first notice how the first leftmost peak is always going to be the highest and always \"all ones\" when expressed in binary. Therefore that peak will have a position of the form 2^n - 1 and will always be the largest such position that is inside the MMR (its position is lesser than the total size). We process iteratively for a MMR of size 11: 2^0 - 1 = 0, and 0 < 11 2^1 - 1 = 1, and 1 < 11 2^2 - 1 = 3, and 3 < 11 2^3 - 1 = 7, and 7 < 11 2^4 - 1 = 15, and 15 is not < 11 (This can also be calculated non-iteratively as 2^(binary logarithm of size + 1) - 1 Therefore the first peak is 7. To find the next peak, we then need to \"jump\" to its right sibling. If that node is not in the MMR (and it won't), take its left child. If that child is not in the MMR either, keep taking its left child until we have a node that exists in our MMR. Once we find that next peak, keep repeating the process until we're at the last node. All these operations are very simple. Jumping to the right sibling of a node at height h is adding 2^(h+1) - 1 to its position. Taking its left child is subtracting 2^h . Finally, once all the positions of the peaks are known, \"bagging\" the peaks consists of hashing them iteratively from the right, using the total size of the MMR as prefix. For a MMR of size N with 3 peaks p1, p2 and p3 we get the final top peak: P = Blake2b(N | Blake2b(N | Node(p3) | Node(p2)) | Node(p1)) Pruning In Grin, a lot of the data that gets hashed and stored in MMRs can eventually be removed. As this happens, the presence of some leaf hashes in the corresponding MMRs become unnecessary and their hash can be removed. When enough leaves are removed, the presence of their parents may become unnecessary as well. We can therefore prune a significant part of a MMR from the removal of its leaves. Pruning a MMR relies on a simple iterative process. X is first initialized as the leaf we wish to prune. Prune X . If X has a sibling, stop here. If 'X' has no sibling, assign the parent of X as X . To visualize the result, starting from our first MMR example and removing leaves [0, 3, 4, 8, 16] leads to the following pruned MMR: Height 3 14 / \\ / \\ / \\ / \\ 2 6 13 / / \\ 1 2 9 12 17 \\ / / \\ / 0 1 7 10 11 15 18 [1] Peter Todd, merkle-mountain-range [2] Wikipedia, Merkle Tree","title":"Merkle Mountain Ranges (MMRs)"},{"location":"technical/building-blocks/merkle-mountain-ranges/#merkle-mountain-ranges","text":"","title":"Merkle Mountain Ranges"},{"location":"technical/building-blocks/merkle-mountain-ranges/#structure","text":"Merkle Mountain Ranges[1] are an alternative to Merkle trees[2]. While the latter relies on perfectly balanced binary trees, the former can be seen either as list of perfectly balance binary trees or a single binary tree that would have been truncated from the top right. A Merkle Mountain Range (MMR) is strictly append-only: elements are added from the left to the right, adding a parent as soon as 2 children exist, filling up the range accordingly. This illustrates a range with 11 inserted leaves and total size 19, where each node is annotated with its order of insertion. Height 3 14 / \\ / \\ / \\ / \\ 2 6 13 / \\ / \\ 1 2 5 9 12 17 / \\ / \\ / \\ / \\ / \\ 0 0 1 3 4 7 8 10 11 15 16 18 This can be represented as a flat list, here storing the height of each node at their position of insertion: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 0 0 1 0 0 1 2 0 0 1 0 0 1 2 3 0 0 1 0 This structure can be fully described simply from its size (19). It's also fairly simple, using fast binary operations, to navigate within a MMR. Given a node's position n , we can compute its height, the position of its parent, its siblings, etc.","title":"Structure"},{"location":"technical/building-blocks/merkle-mountain-ranges/#hashing-and-bagging","text":"Just like with Merkle trees, parent nodes in a MMR have for value the hash of their 2 children. Grin uses the Blake2b hash function throughout, and always prepends the node's position in the MMR before hashing to avoid collisions. So for a leaf l at index n storing data D (in the case of an output, the data is its Pedersen commitment, for example), we have: Node(l) = Blake2b(n | D) And for any parent p at index m : Node(p) = Blake2b(m | Node(left_child(p)) | Node(right_child(p))) Contrarily to a Merkle tree, a MMR generally has no single root by construction so we need a method to compute one (otherwise it would defeat the purpose of using a hash tree). This process is called \"bagging the peaks\" for reasons described in [1]. First, we identify the peaks of the MMR; we will define one method of doing so here. We first write another small example MMR but with the indexes written as binary (instead of decimal), starting from 1: Height 2 111 / \\ 1 11 110 1010 / \\ / \\ / \\ 0 1 10 100 101 1000 1001 1011 This MMR has 11 nodes and its peaks are at position 111 (7), 1010 (10) and 1011 (11). We first notice how the first leftmost peak is always going to be the highest and always \"all ones\" when expressed in binary. Therefore that peak will have a position of the form 2^n - 1 and will always be the largest such position that is inside the MMR (its position is lesser than the total size). We process iteratively for a MMR of size 11: 2^0 - 1 = 0, and 0 < 11 2^1 - 1 = 1, and 1 < 11 2^2 - 1 = 3, and 3 < 11 2^3 - 1 = 7, and 7 < 11 2^4 - 1 = 15, and 15 is not < 11 (This can also be calculated non-iteratively as 2^(binary logarithm of size + 1) - 1 Therefore the first peak is 7. To find the next peak, we then need to \"jump\" to its right sibling. If that node is not in the MMR (and it won't), take its left child. If that child is not in the MMR either, keep taking its left child until we have a node that exists in our MMR. Once we find that next peak, keep repeating the process until we're at the last node. All these operations are very simple. Jumping to the right sibling of a node at height h is adding 2^(h+1) - 1 to its position. Taking its left child is subtracting 2^h . Finally, once all the positions of the peaks are known, \"bagging\" the peaks consists of hashing them iteratively from the right, using the total size of the MMR as prefix. For a MMR of size N with 3 peaks p1, p2 and p3 we get the final top peak: P = Blake2b(N | Blake2b(N | Node(p3) | Node(p2)) | Node(p1))","title":"Hashing and Bagging"},{"location":"technical/building-blocks/merkle-mountain-ranges/#pruning","text":"In Grin, a lot of the data that gets hashed and stored in MMRs can eventually be removed. As this happens, the presence of some leaf hashes in the corresponding MMRs become unnecessary and their hash can be removed. When enough leaves are removed, the presence of their parents may become unnecessary as well. We can therefore prune a significant part of a MMR from the removal of its leaves. Pruning a MMR relies on a simple iterative process. X is first initialized as the leaf we wish to prune. Prune X . If X has a sibling, stop here. If 'X' has no sibling, assign the parent of X as X . To visualize the result, starting from our first MMR example and removing leaves [0, 3, 4, 8, 16] leads to the following pruned MMR: Height 3 14 / \\ / \\ / \\ / \\ 2 6 13 / / \\ 1 2 9 12 17 \\ / / \\ / 0 1 7 10 11 15 18 [1] Peter Todd, merkle-mountain-range [2] Wikipedia, Merkle Tree","title":"Pruning"},{"location":"technical/building-blocks/merkle-proof/","text":"Merkle Proof","title":"Merkle Proof"},{"location":"technical/building-blocks/merkle-proof/#merkle-proof","text":"","title":"Merkle Proof"},{"location":"technical/building-blocks/switch-commitments/","text":"Introduction to Switch Commitments General introduction In cryptography a Commitment (or commitment scheme ) refers to a concept which can be imagined like a box with a lock. You can put something into the box (for example a piece of a paper with a secret number written on it), lock it and give it to another person (or the public). The other person doesn't know yet what's the secret number in the box, but if you decide to publish your secret number later in time and want to prove that this really is the secret which you came up with in the first place (and not a different one) you can prove this simply by giving the key of the box to the other person. They can unlock the box, compare the secret within the box with the secret you just published and can be sure that you didn't change your secret since you locked it. You \" committed \" to the secret number beforehand, meaning you cannot change it between the time of commitment and the time of revealing. Examples Hash Commitment A simple commitment scheme can be realized with a cryptographic hash function. For example: Alice and Bob want to play \"Guess my number\" and Alice comes up with with her really secret number 29 which Bob has to guess in the game, then before the game starts, Alice calculates: hash( 29 + r ) and publishes the result to Bob. The r is a randomly chosen Blinding Factor which is needed because otherwise Bob could just try hashing all the possible numbers for the game and compare the hashes. When the game is finished, Alice simply needs to publish her secret number 29 and the blinding factor r and Bob can calculate the hash himself and easily verify that Alice did not change the secret number during the game. Pedersen Commitment Other, more advanced commitment schemes can have additional properties. For example Mimblewimble and Confidential Transactions (CT) make heavy use of Pedersen Commitments , which are homomorphic commitments. Homomorphic in this context means that (speaking in the \"box\" metaphor from above) you can take two of these locked boxes ( box1 and box2 ) and somehow \" add \" them together, so that you get a single box as result (which still is locked), and if you open this single box later (like in the examples before) the secret it contains, is the sum of the secrets from box1 and box2 . While this \"box\" metaphor no longer seems to be reasonable in the real-world this is perfectly possible using the properties of operations on elliptic curves. Look into Introduction to Mimblewimble for further details on Pedersen Commitments and how they are used in Grin. Properties of commitment schemes: In general for any commitment scheme we can identify two important properties which can be weaker or stronger, depending on the type of commitment scheme: Hidingness (or Confidentiality): How good is the commitment scheme protecting the secret commitment. Or speaking in terms of our example from above: what would an attacker need to open the box (and learn the secret number) without having the key to unlock it? Bindingness: Is it possible at all (or how hard would it be) for an attacker to somehow find a different secret, which would produce the same commitment, so that the attacker could later open the commitment to a different secret, thus breaking the binding of the commitment. Security of these properties: For these two properties different security levels can be identified. The two most important combinations of these are perfectly binding and computationally hiding commitment schemes and computationally binding and perfectly hiding commitment schemes \" Computationally \" binding or hiding means that the property (bindingness/hidingness) is secured by the fact that the underlying mathematical problem is too hard to be solved with existing computing power in reasonable time (i.e. not breakable today as computational resources are bound in the real world). \" Perfectly \" binding or hiding means that even with infinite computing power it would be impossible to break the property (bindingness/hidingness). Mutual exclusivity: It is important to realize that it's impossible that any commitment scheme can be perfectly binding and perfectly hiding at the same time. This can be easily shown with a thought experiment: Imagine an attacker having infinite computing power, he could simply generate a commitment for all possible values (and blinding factors) until finding a pair that outputs the same commitment. If we further assume the commitment scheme is perfectly binding (meaning there cannot be two different values leading to the same commitment) this uniquely would identify the value within the commitment, thus breaking the hidingness. The same is true the other way around. If a commitment scheme is perfectly hiding there must exist several input values resulting in the same commitment (otherwise an attacker with infinite computing power could just try all possible values as described above). This concludes that the commitment scheme cannot be perfectly binding . Always a compromise The key take-away point is this: it's always a compromise , you can never have both properties ( hidingness and bindingness ) with perfect security. If one is perfectly secure then the other can be at most computationally secure (and the other way around). Considerations for cryptocurrencies Which roles do these properties play in the design of cryptocurrencies? Hidingness : In privacy oriented cryptocurrencies like Grin, commitment schemes are used to secure the contents of transactions. The sender commits to an amount of coins he sends, but for the general public the concrete amount should remain private (protected by the hidingness property of the commitment scheme). Bindingness : At the same time no transaction creator should ever be able to change his commitment to a different transaction amount later in time. If this would be possible, an attacker could spend more coins than previously committed to in an UTXO (unspent transaction output) and therefore inflate coins out of thin air. Even worse, as the amounts are hidden, this could go undetected. So there is a valid interest in having both of these properties always secured and never be violated. Even with the intent being that both of these properties will hold for the lifetime of a cryptocurrency, still a choice has to be made about which commitment scheme to use. A hard choice? Which one of these two properties needs to be perfectly safe and for which one it would be sufficient to be computationally safe? Or in other words: in case of a disaster, if the commitment scheme unexpectedly gets broken, which one of the two properties should be valued higher? Economical soundness (no hidden inflation possible) or ensured privacy (privacy will be preserved)? This seems like a hard to choice to make. If we look closer into this we realize that the commitment scheme only needs to be perfectly binding at the point in time when the scheme actually gets broken. Until then it will be safe even if it's only computationally binding. At the same time a privacy-oriented cryptocurrency needs to ensure the hidingness property forever . Unlike the binding property, which only is important at the time when a transaction is created and will not affect past transactions, the hidingness property must be ensured at all times. Otherwise, in the unfortunate case should the commitment scheme be broken, an attacker could go back in the chain and unblind past transactions, thus break the privacy property retroactively. Properties of Pedersen Commitments Pedersen Commitments are computationally binding and perfectly hiding as for a given commitment to the value v : v*H + r*G there may exist a pair of different values r1 and v1 such that the sum will be the same. Even if you have infinite computing power and could try all possible values, you would not be able to tell which one is the original one (thus perfectly hiding ). Introducing Switch Commitments So what can be done if the bindingness of the Pedersen Commitment unexpectedly gets broken? In general a cryptocurrency confronted with a broken commitment scheme could choose to change the scheme in use, but the problem with this approach would be that it requires to create new transaction outputs using the new scheme to make funds secure again. This would require every coin holder to move his coins into new transaction outputs. If coins are not moved into new outputs, they will not profit from the security of the new commitment scheme. Also, this has to happen before the scheme gets actually broken in the wild, otherwise the existing UTXOs no longer can be assumed to contain correct values. In this situation Switch Commitments offer a neat solution. These type of commitments allow changing the properties of the commitments just by changing the revealing / validating procedure without changing the way commitments are created. (You \" switch \" to a new validation scheme which is backwards compatible with commitments created long before the actual \" switch \"). How does this work in detail First let's introduce a new commitment scheme: The ElGamal commitment scheme is a commitment scheme similiar to Pedersen Commitments and it's perfectly binding (but only computationally hiding as we can never have both). It looks very similar to a Pedersen Commitment, with the addition of a new element, calculated by multiplying the blinding factor r with another generator point J : v*H + r*G , r*J So if we store the additional field r*J and ignore it for now, we can treat it like Pedersen Commitments, until we decide to also validate the full ElGamal commitment at some time in future. This is exactly what was implemented in an earlier version of Grin , before mainnet was launched. In detail: the hashed value of r*J ( switch_commit_hash ) was added to the transaction output, but this came with the burden of increasing the size of each output by 32 bytes. Fortunately, later on the Mimblewimble mailinglist Tim Ruffing came up with a really beautiful idea (initially suggested by Pieter Wuille), which offers the same advantages but doesn't need this extra storage of an additional element per transaction output: The idea is the following: A normal Pedersen commitment looks like this: v*H + r*G ( v is value of the input/output, r is a truly random blinding factor, and H and G are two generator points on the elliptic curve). If we adapt this by having r not being random itself, but using another random number r' and create the Pedersen Commitment: v*H + r*G such that: r = r' + hash( v*H + r'*G , r'*J ) (using the additional third generation point J on the curve) then r still is perfectly valid as a blinding factor, as it's still randomly distributed, but now we see that the part within the brackets of the hash function ( v*H + r'*G , r'*J ) is an ElGamal commitment . This neat idea lead to the removal of the switch commitment hash from the outputs in this (and following) pull requests as now it could be easily included into the Pedersen Commitments. This is how it is currently implemented in Grin. Pedersen commitments are used for the Confidential Transaction but instead of choosing the blinding factor r only by random, it is calculated by adding the hash of an ElGamal commitment to a random r' (see here in main_impl.h#L267 ). In general switch commitments were first described in the paper \"Switch Commitments: A Safety Switch for Confidential Transactions\" ). The \"switch\" in the name comes from the fact that you can virtually flip a \"switch\" in the future and simply by changing the validation procedure you can change the strength of the bindingness and hidingness property of your commitments and this even works in a backwards compatible way with commitments created today. Conclusion Grin uses Pedersen Commitments - like other privacy cryptocurrencies do as well - with the only difference that the random blinding factor r is created using the ElGamal commitment scheme. This might not seem like a big change on a first look, but it provides an important safety measure: Pedersen Commitments are already perfectly hiding so whatever happens, privacy will never be at risk without requiring any action from users. But in case of a disaster if the bindingness of the commitment scheme gets broken, then switch commitments can be enabled (via a soft fork) requiring that all new transactions prove that their commitment is not breaking the bindingness by validating the full ElGamal commitment. But in this case users would still have a choice: they can decide to continue to create new transactions, even if this might compromise their privacy (only on their last UTXOs) as the ElGamal commitment scheme is only computationally hiding, but at least they would still have access to their coins or users can decide to just leave the money alone, walk away and make no more transactions (but preserve their privacy, as their old transactions only validated the Pedersen commitment which is perfectly hiding) There are many cases where a privacy leak is much more dangerous to one's life than some cryptocurrency might be worth. But this is a decision that should be left up to the individual user and switch commitments enable this type of choice. It should be made clear that this is a safety measure meant to be enabled in case of a disaster. If advances in computing would put the hardness of the discrete log problem in question, a lot of other cryptographic systems, including other cryptocurrencies, will be in urgent need of updating their primitives to a future-proof system. The switch commitments just provide an additional layer of security if the bindingness of Pedersen commitments ever breaks unexpectedly.","title":"Switch Commitments"},{"location":"technical/building-blocks/switch-commitments/#introduction-to-switch-commitments","text":"","title":"Introduction to Switch Commitments"},{"location":"technical/building-blocks/switch-commitments/#general-introduction","text":"In cryptography a Commitment (or commitment scheme ) refers to a concept which can be imagined like a box with a lock. You can put something into the box (for example a piece of a paper with a secret number written on it), lock it and give it to another person (or the public). The other person doesn't know yet what's the secret number in the box, but if you decide to publish your secret number later in time and want to prove that this really is the secret which you came up with in the first place (and not a different one) you can prove this simply by giving the key of the box to the other person. They can unlock the box, compare the secret within the box with the secret you just published and can be sure that you didn't change your secret since you locked it. You \" committed \" to the secret number beforehand, meaning you cannot change it between the time of commitment and the time of revealing.","title":"General introduction"},{"location":"technical/building-blocks/switch-commitments/#examples","text":"","title":"Examples"},{"location":"technical/building-blocks/switch-commitments/#hash-commitment","text":"A simple commitment scheme can be realized with a cryptographic hash function. For example: Alice and Bob want to play \"Guess my number\" and Alice comes up with with her really secret number 29 which Bob has to guess in the game, then before the game starts, Alice calculates: hash( 29 + r ) and publishes the result to Bob. The r is a randomly chosen Blinding Factor which is needed because otherwise Bob could just try hashing all the possible numbers for the game and compare the hashes. When the game is finished, Alice simply needs to publish her secret number 29 and the blinding factor r and Bob can calculate the hash himself and easily verify that Alice did not change the secret number during the game.","title":"Hash Commitment"},{"location":"technical/building-blocks/switch-commitments/#pedersen-commitment","text":"Other, more advanced commitment schemes can have additional properties. For example Mimblewimble and Confidential Transactions (CT) make heavy use of Pedersen Commitments , which are homomorphic commitments. Homomorphic in this context means that (speaking in the \"box\" metaphor from above) you can take two of these locked boxes ( box1 and box2 ) and somehow \" add \" them together, so that you get a single box as result (which still is locked), and if you open this single box later (like in the examples before) the secret it contains, is the sum of the secrets from box1 and box2 . While this \"box\" metaphor no longer seems to be reasonable in the real-world this is perfectly possible using the properties of operations on elliptic curves. Look into Introduction to Mimblewimble for further details on Pedersen Commitments and how they are used in Grin.","title":"Pedersen Commitment"},{"location":"technical/building-blocks/switch-commitments/#properties-of-commitment-schemes","text":"In general for any commitment scheme we can identify two important properties which can be weaker or stronger, depending on the type of commitment scheme: Hidingness (or Confidentiality): How good is the commitment scheme protecting the secret commitment. Or speaking in terms of our example from above: what would an attacker need to open the box (and learn the secret number) without having the key to unlock it? Bindingness: Is it possible at all (or how hard would it be) for an attacker to somehow find a different secret, which would produce the same commitment, so that the attacker could later open the commitment to a different secret, thus breaking the binding of the commitment.","title":"Properties of commitment schemes:"},{"location":"technical/building-blocks/switch-commitments/#security-of-these-properties","text":"For these two properties different security levels can be identified. The two most important combinations of these are perfectly binding and computationally hiding commitment schemes and computationally binding and perfectly hiding commitment schemes \" Computationally \" binding or hiding means that the property (bindingness/hidingness) is secured by the fact that the underlying mathematical problem is too hard to be solved with existing computing power in reasonable time (i.e. not breakable today as computational resources are bound in the real world). \" Perfectly \" binding or hiding means that even with infinite computing power it would be impossible to break the property (bindingness/hidingness).","title":"Security of these properties:"},{"location":"technical/building-blocks/switch-commitments/#mutual-exclusivity","text":"It is important to realize that it's impossible that any commitment scheme can be perfectly binding and perfectly hiding at the same time. This can be easily shown with a thought experiment: Imagine an attacker having infinite computing power, he could simply generate a commitment for all possible values (and blinding factors) until finding a pair that outputs the same commitment. If we further assume the commitment scheme is perfectly binding (meaning there cannot be two different values leading to the same commitment) this uniquely would identify the value within the commitment, thus breaking the hidingness. The same is true the other way around. If a commitment scheme is perfectly hiding there must exist several input values resulting in the same commitment (otherwise an attacker with infinite computing power could just try all possible values as described above). This concludes that the commitment scheme cannot be perfectly binding .","title":"Mutual exclusivity:"},{"location":"technical/building-blocks/switch-commitments/#always-a-compromise","text":"The key take-away point is this: it's always a compromise , you can never have both properties ( hidingness and bindingness ) with perfect security. If one is perfectly secure then the other can be at most computationally secure (and the other way around).","title":"Always a compromise"},{"location":"technical/building-blocks/switch-commitments/#considerations-for-cryptocurrencies","text":"Which roles do these properties play in the design of cryptocurrencies? Hidingness : In privacy oriented cryptocurrencies like Grin, commitment schemes are used to secure the contents of transactions. The sender commits to an amount of coins he sends, but for the general public the concrete amount should remain private (protected by the hidingness property of the commitment scheme). Bindingness : At the same time no transaction creator should ever be able to change his commitment to a different transaction amount later in time. If this would be possible, an attacker could spend more coins than previously committed to in an UTXO (unspent transaction output) and therefore inflate coins out of thin air. Even worse, as the amounts are hidden, this could go undetected. So there is a valid interest in having both of these properties always secured and never be violated. Even with the intent being that both of these properties will hold for the lifetime of a cryptocurrency, still a choice has to be made about which commitment scheme to use.","title":"Considerations for cryptocurrencies"},{"location":"technical/building-blocks/switch-commitments/#a-hard-choice","text":"Which one of these two properties needs to be perfectly safe and for which one it would be sufficient to be computationally safe? Or in other words: in case of a disaster, if the commitment scheme unexpectedly gets broken, which one of the two properties should be valued higher? Economical soundness (no hidden inflation possible) or ensured privacy (privacy will be preserved)? This seems like a hard to choice to make. If we look closer into this we realize that the commitment scheme only needs to be perfectly binding at the point in time when the scheme actually gets broken. Until then it will be safe even if it's only computationally binding. At the same time a privacy-oriented cryptocurrency needs to ensure the hidingness property forever . Unlike the binding property, which only is important at the time when a transaction is created and will not affect past transactions, the hidingness property must be ensured at all times. Otherwise, in the unfortunate case should the commitment scheme be broken, an attacker could go back in the chain and unblind past transactions, thus break the privacy property retroactively.","title":"A hard choice?"},{"location":"technical/building-blocks/switch-commitments/#properties-of-pedersen-commitments","text":"Pedersen Commitments are computationally binding and perfectly hiding as for a given commitment to the value v : v*H + r*G there may exist a pair of different values r1 and v1 such that the sum will be the same. Even if you have infinite computing power and could try all possible values, you would not be able to tell which one is the original one (thus perfectly hiding ).","title":"Properties of Pedersen Commitments"},{"location":"technical/building-blocks/switch-commitments/#introducing-switch-commitments","text":"So what can be done if the bindingness of the Pedersen Commitment unexpectedly gets broken? In general a cryptocurrency confronted with a broken commitment scheme could choose to change the scheme in use, but the problem with this approach would be that it requires to create new transaction outputs using the new scheme to make funds secure again. This would require every coin holder to move his coins into new transaction outputs. If coins are not moved into new outputs, they will not profit from the security of the new commitment scheme. Also, this has to happen before the scheme gets actually broken in the wild, otherwise the existing UTXOs no longer can be assumed to contain correct values. In this situation Switch Commitments offer a neat solution. These type of commitments allow changing the properties of the commitments just by changing the revealing / validating procedure without changing the way commitments are created. (You \" switch \" to a new validation scheme which is backwards compatible with commitments created long before the actual \" switch \").","title":"Introducing Switch Commitments"},{"location":"technical/building-blocks/switch-commitments/#how-does-this-work-in-detail","text":"First let's introduce a new commitment scheme: The ElGamal commitment scheme is a commitment scheme similiar to Pedersen Commitments and it's perfectly binding (but only computationally hiding as we can never have both). It looks very similar to a Pedersen Commitment, with the addition of a new element, calculated by multiplying the blinding factor r with another generator point J : v*H + r*G , r*J So if we store the additional field r*J and ignore it for now, we can treat it like Pedersen Commitments, until we decide to also validate the full ElGamal commitment at some time in future. This is exactly what was implemented in an earlier version of Grin , before mainnet was launched. In detail: the hashed value of r*J ( switch_commit_hash ) was added to the transaction output, but this came with the burden of increasing the size of each output by 32 bytes. Fortunately, later on the Mimblewimble mailinglist Tim Ruffing came up with a really beautiful idea (initially suggested by Pieter Wuille), which offers the same advantages but doesn't need this extra storage of an additional element per transaction output: The idea is the following: A normal Pedersen commitment looks like this: v*H + r*G ( v is value of the input/output, r is a truly random blinding factor, and H and G are two generator points on the elliptic curve). If we adapt this by having r not being random itself, but using another random number r' and create the Pedersen Commitment: v*H + r*G such that: r = r' + hash( v*H + r'*G , r'*J ) (using the additional third generation point J on the curve) then r still is perfectly valid as a blinding factor, as it's still randomly distributed, but now we see that the part within the brackets of the hash function ( v*H + r'*G , r'*J ) is an ElGamal commitment . This neat idea lead to the removal of the switch commitment hash from the outputs in this (and following) pull requests as now it could be easily included into the Pedersen Commitments. This is how it is currently implemented in Grin. Pedersen commitments are used for the Confidential Transaction but instead of choosing the blinding factor r only by random, it is calculated by adding the hash of an ElGamal commitment to a random r' (see here in main_impl.h#L267 ). In general switch commitments were first described in the paper \"Switch Commitments: A Safety Switch for Confidential Transactions\" ). The \"switch\" in the name comes from the fact that you can virtually flip a \"switch\" in the future and simply by changing the validation procedure you can change the strength of the bindingness and hidingness property of your commitments and this even works in a backwards compatible way with commitments created today.","title":"How does this work in detail"},{"location":"technical/building-blocks/switch-commitments/#conclusion","text":"Grin uses Pedersen Commitments - like other privacy cryptocurrencies do as well - with the only difference that the random blinding factor r is created using the ElGamal commitment scheme. This might not seem like a big change on a first look, but it provides an important safety measure: Pedersen Commitments are already perfectly hiding so whatever happens, privacy will never be at risk without requiring any action from users. But in case of a disaster if the bindingness of the commitment scheme gets broken, then switch commitments can be enabled (via a soft fork) requiring that all new transactions prove that their commitment is not breaking the bindingness by validating the full ElGamal commitment. But in this case users would still have a choice: they can decide to continue to create new transactions, even if this might compromise their privacy (only on their last UTXOs) as the ElGamal commitment scheme is only computationally hiding, but at least they would still have access to their coins or users can decide to just leave the money alone, walk away and make no more transactions (but preserve their privacy, as their old transactions only validated the Pedersen commitment which is perfectly hiding) There are many cases where a privacy leak is much more dangerous to one's life than some cryptocurrency might be worth. But this is a decision that should be left up to the individual user and switch commitments enable this type of choice. It should be made clear that this is a safety measure meant to be enabled in case of a disaster. If advances in computing would put the hardness of the discrete log problem in question, a lot of other cryptographic systems, including other cryptocurrencies, will be in urgent need of updating their primitives to a future-proof system. The switch commitments just provide an additional layer of security if the bindingness of Pedersen commitments ever breaks unexpectedly.","title":"Conclusion"},{"location":"technical/miscellaneous/contracts/","text":"Contracts This document describes smart contracts that can be setup using Grin even though the Grin chain does not support scripting. All these contracts rely on a few basic features that are built in the chain and compose them in increasingly clever ways. None of those constructs are fully original or invented by the authors of this document or the Grin development team. Most of the credit should be attributed to a long list of cryptographers and researchers. To name just a few: Torben Pryds Pedersen, Gregory Maxwell, Andrew Poelstra, John Tromp, Claus Peter Schnorr. We apologize in advance for all those we couldn't name and recognize that most computer science discoveries are incremental. Built-Ins This section is meant as a reminder of some crucial features of the Grin chain. We assume some prior reading as to how these are constructed and used. Pedersen Commitments All outputs include a Pedersen commitment of the form r*G + v*H with r the blinding factor, v the value, and G and H two distinct generator points on the same curve group. Aggregate Signatures (a.k.a. Schnorr, MuSig) We suppose we have the SHA256 hash function and the same G curve as above. In its simplest form, an aggregate signature is built from: the message M to sign, in our case the transaction fee a private key x , with its matching public key x*G a nonce k just used for the purpose of building the signature We build the challenge e = SHA256(M | k*G | x*G) , and the scalar s = k + e * x . The full aggregate signature is then the pair (s, k*G) . The signature can be checked using the public key x*G , re-calculating e using M and k*G from the 2nd part of the signature pair and by verifying that s , the first part of the signature pair, satisfies: s*G = k*G + e * x*G In this simple case of someone sending a transaction to a receiver they trust (see later for the trustless case), an aggregate signature can be directly built for a Grin transaction by taking the above private key x to be the sum of output blinding factors minus the sum of input blinding factors. The resulting kernel is assembled from the aggregate signature generated using r and the public key r*G , and allows to verify non-inflation for all Grin transactions (and signs the fees). Because these signatures are built simply from a scalar and a public key, they can be used to construct a variety of contracts using \"simple\" arithmetic. (Absolute) Timelocked Transactions Analogous to Bitcoin nLockTime . A transaction can be time-locked with a few simple modifications: the message M to sign becomes the lock_height h at which the transaction becomes spendable appended to the fee M = fee | h the lock height h is included in the transaction kernel a block with a kernel that includes a lock height greater than the current block height is rejected (Relative) Timelocked Transactions We can extend the concept of an absolute locktime on a tx by including a (kernel) commitment that we can define the lock_height relative to. The lock_height would be relative to the block height where the referenced kernel was first included in the chain state. Tx2 can then be restricted such that it would only be valid to include it in a block once h blocks have passed after first seeing Tx1 (via the referenced kernel commitment). the message M to sign would need to include the following - the fee as before the lock_height h (as before but interpreted as a relative value) a referenced kernel commitment C M = fee | h | C For Tx2 to be accepted it would also need to include a Merkle proof identifying the block including C from Tx1. This proves the relative lock_height requirement has been met. Derived Contracts Trustless Transactions An aggregate (Schnorr) signature involving a single party is relatively simple but does not demonstrate the full flexibility of the construction. We show here how to generalize it for use in outputs involving multiple parties. As constructed in section 1.2, an aggregate signature requires trusting the receiving party. As Grin outputs are completely obscured by Pedersen Commitments, one cannot prove money was actually sent to the right party, hence a receiver could claim not having received anything. To solve this issue, we require the receiver to collaborate with the sender in building a transaction and specifically its kernel signature. Alice wants to pay Bob in grins. She starts the transaction building process: Alice selects her inputs and builds her change output. The sum of all blinding factors (change output minus inputs) is rs . Alice picks a random nonce ks and sends her partial transaction, ks*G and rs*G to Bob. Bob picks his own random nonce kr and the blinding factor for his output rr . Using rr , Bob adds his output to the transaction. Bob computes the message M = fee | lock_height , the Schnorr challenge e = SHA256(M | kr*G + ks*G | rr*G + rs*G) and finally his side of the signature sr = kr + e * rr . Bob sends sr , kr*G and rr*G to Alice. Alice computes e just like Bob did and can check that sr*G = kr*G + e*rr*G . Alice sends her side of the signature ss = ks + e * rs to Bob. Bob validates ss*G just like Alice did for sr*G in step 6 and can produce the final signature s = (ss + sr, ks*G + kr*G) as well as the final transaction kernel including s and the public key rr*G + rs*G . This protocol requires 3 data exchanges (Alice to Bob, Bob back to Alice, and finally Alice to Bob) and is therefore said to be interactive. However the interaction can be done over any medium and in any period of time, including the pony express over 2 weeks. This protocol can also be generalized to any number i of parties. On the first round, all the ki*G and ri*G are shared. On the 2nd round, everyone can compute e = SHA256(M | sum(ki*G) | sum(ri*G)) and their own signature si . Finally, a finalizing party can then gather all the partial signatures si , validate them and produce s = (sum(si), sum(ki*G)) . Multiparty Outputs (multisig) We describe here a way to build a transaction with an output that can only be spent when multiple parties approve it. This construction is very similar to the previous setup for trustless transactions, however in this case both the signature and a Pedersen Commitment need to be aggregated. This time, Alice wants to send funds such that both Bob and her need to agree to spend. Alice builds the transaction normally and adds the multiparty output such that: Bob picks a blinding factor rb and sends rb*G to Alice. Alice picks a blinding factor ra and builds the commitment C = ra*G + rb*G + v*H . She sends the commitment to Bob. Bob creates a range proof for v using C and rb and sends it to Alice. Alice generates her own range proof, aggregates it with Bob, finalizing the multiparty output Oab . The kernel is built following the same procedure as for Trustless Transactions. We observe that for that new output Oab , neither party know the whole blinding factor. To be able to build a transaction spending Oab, someone would need to know ra + rb to produce a kernel signature. To produce that spending kernel, Alice and Bob need to collaborate. This, again, is done using a protocol very close to Trustless Transactions. Multiparty Timelocks This contract is a building block for multiple other contracts. Here, Alice agrees to lock some funds to start a financial interaction with Bob and prove to Bob she has funds. The setup is the following: Alice builds a a 2-of-2 multiparty transaction with an output she shares with Bob, however she does not participate in building the kernel signature yet. Bob builds a refund transaction with Alice that sends the funds back to Alice using a timelock (for example 1440 blocks ahead, about 24h). Alice and Bob finish the 2-of-2 transaction by building the corresponding kernel and broadcast it. Now Alice and Bob are free to build additional transactions distributing the funds locked in the 2-of-2 output in any way they see fit. If Bob refuses to cooperate, Alice just needs to broadcast her refund transaction after the time lock expires. This contract can be trivially used for unidirectional payment channels. Conditional Output Timelocks Analogous to Bitcoin CheckLockTimeVerify . We currently have unconditional lock_heights on txs (tx is not valid and will not be accepted until lock_height has passed). Private keys can be summed together. Key 3 = Key 1 + Key 2 Commitments can be summed together. C 3 = C 1 + C 2 Given unconditional locktimes on txs we can leverage these to give us conditional locktimes on outputs by \"entangling\" two outputs on two related txs together. We can construct two txs (Tx 1 , Tx 2 ) with two entangled outputs Out 1 and Out 2 such that - Out 1 (commitment C 1 ) is from Tx 1 and built using Key 1 Out 2 (commitment C 2 ) is from Tx 2 and built using Key 2 Tx 2 has an unconditional lock_height on it If we do this (and we can manage the keys as necessary) - Out 1 + Out 2 can only be spent as a pair using Key 3 They can only be spent after lock_height from Tx 2 Tx 1 (containing Out 1 ) can be broadcast, accepted and confirmed on-chain immediately. Tx 2 cannot be broadcast and accepted until lock_height has passed. So if Alice only knows K 3 and does not know Key 1 or Key 2 , then Out 1 can only be spent by Alice after lock_height has passed. If Bob on the other hand knows Key 2 then Out 1 can be spent by Bob immediately. We have a conditional timelock on Out 1 (confirmed, on-chain) where it can be spent either with Key 3 (after lock_height), or Key 2 immediately. (Relative) Conditional Output Timelocks Analogous to Bitcoin CheckSequenceVerify . By combining \"Conditional Timelock on Output\" with \"(Relative) Timelocked Transactions\" we can encumber a confirmed output with a relative timelock (relative to a related tx kernel). Tx 1 (containing Out 1 ) can be broadcast, accepted and confirmed on-chain immediately. Tx 2 cannot be broadcast and accepted until the relative lock_height has passed, relative to the referenced kernel from the earlier Tx 1 . Atomic Swap This setup can work on Bitcoin, Ethereum and likely other chains. It relies on a time locked contract combined with a check for 2 public keys. On Bitcoin this would be a 2-of-2 multisig, one public key being Alice's, the second being the hash of a preimage that Bob has to reveal. In this setup, we consider public key derivation x*G to be the hash function and by Bob revealing x , Alice can then produce an adequate signature proving she knows x (in addition to her own private key). Alice has grins and Bob has bitcoin. They would like to swap. We assume Bob created an output on the Bitcoin blockchain that allows spending either by Alice if she learns a hash pre-image x , or by Bob after time Tb . Alice is ready to send her grins to Bob if he reveals x . First, Alice sends her grins to a multiparty timelock contract with a refund time Ta < Tb . To send the 2-of-2 output to Bob and execute the swap, Alice and Bob start as if they were building a normal trustless transaction as specified in section 2.1. Alice picks a random nonce ks and her blinding sum rs and sends ks*G and rs*G to Bob. Bob picks a random blinding factor rr and a random nonce kr . However this time, instead of simply sending sr = kr + e * rr with his rr*G and kr*G , Bob sends sr' = kr + x + e * rr as well as x*G . Alice can validate that sr'*G = kr*G + x*G + rr*G . She can also check that Bob has money locked with x*G on the other chain. Alice sends back her ss = ks + e * xs as she normally would, now that she can also compute e = SHA256(M | ks*G + kr*G) . To complete the signature, Bob computes sr = kr + e * rr and the final signature is (sr + ss, kr*G + ks*G) . As soon as Bob broadcasts the final transaction to get his new grins, Alice can compute sr' - sr to get x . Notes on the Bitcoin setup Prior to completing the atomic swap, Bob needs to know Alice's public key. Bob would then create an output on the Bitcoin blockchain with a 2-of-2 multisig similar to alice_pubkey secret_pubkey 2 OP_CHECKMULTISIG . This should be wrapped in an OP_IF so Bob can get his money back after an agreed-upon time and all of this can even be wrapped in a P2SH. Here secret_pubkey is x*G from the previous section. To verify the output, Alice would take x*G , recreate the bitcoin script, hash it and check that her hash matches what's in the P2SH (step 2 in previous section). Once she gets x (step 6), she can build the 2 signatures necessary to spend the 2-of-2, having both private keys, and get her bitcoin. \"Relative Timelocks\" (Lightning Network) See No Recent Duplicate (NRD) transaction kernels RFC for more details.","title":"Contracts"},{"location":"technical/miscellaneous/contracts/#contracts","text":"This document describes smart contracts that can be setup using Grin even though the Grin chain does not support scripting. All these contracts rely on a few basic features that are built in the chain and compose them in increasingly clever ways. None of those constructs are fully original or invented by the authors of this document or the Grin development team. Most of the credit should be attributed to a long list of cryptographers and researchers. To name just a few: Torben Pryds Pedersen, Gregory Maxwell, Andrew Poelstra, John Tromp, Claus Peter Schnorr. We apologize in advance for all those we couldn't name and recognize that most computer science discoveries are incremental.","title":"Contracts"},{"location":"technical/miscellaneous/contracts/#built-ins","text":"This section is meant as a reminder of some crucial features of the Grin chain. We assume some prior reading as to how these are constructed and used.","title":"Built-Ins"},{"location":"technical/miscellaneous/contracts/#pedersen-commitments","text":"All outputs include a Pedersen commitment of the form r*G + v*H with r the blinding factor, v the value, and G and H two distinct generator points on the same curve group.","title":"Pedersen Commitments"},{"location":"technical/miscellaneous/contracts/#aggregate-signatures-aka-schnorr-musig","text":"We suppose we have the SHA256 hash function and the same G curve as above. In its simplest form, an aggregate signature is built from: the message M to sign, in our case the transaction fee a private key x , with its matching public key x*G a nonce k just used for the purpose of building the signature We build the challenge e = SHA256(M | k*G | x*G) , and the scalar s = k + e * x . The full aggregate signature is then the pair (s, k*G) . The signature can be checked using the public key x*G , re-calculating e using M and k*G from the 2nd part of the signature pair and by verifying that s , the first part of the signature pair, satisfies: s*G = k*G + e * x*G In this simple case of someone sending a transaction to a receiver they trust (see later for the trustless case), an aggregate signature can be directly built for a Grin transaction by taking the above private key x to be the sum of output blinding factors minus the sum of input blinding factors. The resulting kernel is assembled from the aggregate signature generated using r and the public key r*G , and allows to verify non-inflation for all Grin transactions (and signs the fees). Because these signatures are built simply from a scalar and a public key, they can be used to construct a variety of contracts using \"simple\" arithmetic.","title":"Aggregate Signatures (a.k.a. Schnorr, MuSig)"},{"location":"technical/miscellaneous/contracts/#absolute-timelocked-transactions","text":"Analogous to Bitcoin nLockTime . A transaction can be time-locked with a few simple modifications: the message M to sign becomes the lock_height h at which the transaction becomes spendable appended to the fee M = fee | h the lock height h is included in the transaction kernel a block with a kernel that includes a lock height greater than the current block height is rejected","title":"(Absolute) Timelocked Transactions"},{"location":"technical/miscellaneous/contracts/#relative-timelocked-transactions","text":"We can extend the concept of an absolute locktime on a tx by including a (kernel) commitment that we can define the lock_height relative to. The lock_height would be relative to the block height where the referenced kernel was first included in the chain state. Tx2 can then be restricted such that it would only be valid to include it in a block once h blocks have passed after first seeing Tx1 (via the referenced kernel commitment). the message M to sign would need to include the following - the fee as before the lock_height h (as before but interpreted as a relative value) a referenced kernel commitment C M = fee | h | C For Tx2 to be accepted it would also need to include a Merkle proof identifying the block including C from Tx1. This proves the relative lock_height requirement has been met.","title":"(Relative) Timelocked Transactions"},{"location":"technical/miscellaneous/contracts/#derived-contracts","text":"","title":"Derived Contracts"},{"location":"technical/miscellaneous/contracts/#trustless-transactions","text":"An aggregate (Schnorr) signature involving a single party is relatively simple but does not demonstrate the full flexibility of the construction. We show here how to generalize it for use in outputs involving multiple parties. As constructed in section 1.2, an aggregate signature requires trusting the receiving party. As Grin outputs are completely obscured by Pedersen Commitments, one cannot prove money was actually sent to the right party, hence a receiver could claim not having received anything. To solve this issue, we require the receiver to collaborate with the sender in building a transaction and specifically its kernel signature. Alice wants to pay Bob in grins. She starts the transaction building process: Alice selects her inputs and builds her change output. The sum of all blinding factors (change output minus inputs) is rs . Alice picks a random nonce ks and sends her partial transaction, ks*G and rs*G to Bob. Bob picks his own random nonce kr and the blinding factor for his output rr . Using rr , Bob adds his output to the transaction. Bob computes the message M = fee | lock_height , the Schnorr challenge e = SHA256(M | kr*G + ks*G | rr*G + rs*G) and finally his side of the signature sr = kr + e * rr . Bob sends sr , kr*G and rr*G to Alice. Alice computes e just like Bob did and can check that sr*G = kr*G + e*rr*G . Alice sends her side of the signature ss = ks + e * rs to Bob. Bob validates ss*G just like Alice did for sr*G in step 6 and can produce the final signature s = (ss + sr, ks*G + kr*G) as well as the final transaction kernel including s and the public key rr*G + rs*G . This protocol requires 3 data exchanges (Alice to Bob, Bob back to Alice, and finally Alice to Bob) and is therefore said to be interactive. However the interaction can be done over any medium and in any period of time, including the pony express over 2 weeks. This protocol can also be generalized to any number i of parties. On the first round, all the ki*G and ri*G are shared. On the 2nd round, everyone can compute e = SHA256(M | sum(ki*G) | sum(ri*G)) and their own signature si . Finally, a finalizing party can then gather all the partial signatures si , validate them and produce s = (sum(si), sum(ki*G)) .","title":"Trustless Transactions"},{"location":"technical/miscellaneous/contracts/#multiparty-outputs-multisig","text":"We describe here a way to build a transaction with an output that can only be spent when multiple parties approve it. This construction is very similar to the previous setup for trustless transactions, however in this case both the signature and a Pedersen Commitment need to be aggregated. This time, Alice wants to send funds such that both Bob and her need to agree to spend. Alice builds the transaction normally and adds the multiparty output such that: Bob picks a blinding factor rb and sends rb*G to Alice. Alice picks a blinding factor ra and builds the commitment C = ra*G + rb*G + v*H . She sends the commitment to Bob. Bob creates a range proof for v using C and rb and sends it to Alice. Alice generates her own range proof, aggregates it with Bob, finalizing the multiparty output Oab . The kernel is built following the same procedure as for Trustless Transactions. We observe that for that new output Oab , neither party know the whole blinding factor. To be able to build a transaction spending Oab, someone would need to know ra + rb to produce a kernel signature. To produce that spending kernel, Alice and Bob need to collaborate. This, again, is done using a protocol very close to Trustless Transactions.","title":"Multiparty Outputs (multisig)"},{"location":"technical/miscellaneous/contracts/#multiparty-timelocks","text":"This contract is a building block for multiple other contracts. Here, Alice agrees to lock some funds to start a financial interaction with Bob and prove to Bob she has funds. The setup is the following: Alice builds a a 2-of-2 multiparty transaction with an output she shares with Bob, however she does not participate in building the kernel signature yet. Bob builds a refund transaction with Alice that sends the funds back to Alice using a timelock (for example 1440 blocks ahead, about 24h). Alice and Bob finish the 2-of-2 transaction by building the corresponding kernel and broadcast it. Now Alice and Bob are free to build additional transactions distributing the funds locked in the 2-of-2 output in any way they see fit. If Bob refuses to cooperate, Alice just needs to broadcast her refund transaction after the time lock expires. This contract can be trivially used for unidirectional payment channels.","title":"Multiparty Timelocks"},{"location":"technical/miscellaneous/contracts/#conditional-output-timelocks","text":"Analogous to Bitcoin CheckLockTimeVerify . We currently have unconditional lock_heights on txs (tx is not valid and will not be accepted until lock_height has passed). Private keys can be summed together. Key 3 = Key 1 + Key 2 Commitments can be summed together. C 3 = C 1 + C 2 Given unconditional locktimes on txs we can leverage these to give us conditional locktimes on outputs by \"entangling\" two outputs on two related txs together. We can construct two txs (Tx 1 , Tx 2 ) with two entangled outputs Out 1 and Out 2 such that - Out 1 (commitment C 1 ) is from Tx 1 and built using Key 1 Out 2 (commitment C 2 ) is from Tx 2 and built using Key 2 Tx 2 has an unconditional lock_height on it If we do this (and we can manage the keys as necessary) - Out 1 + Out 2 can only be spent as a pair using Key 3 They can only be spent after lock_height from Tx 2 Tx 1 (containing Out 1 ) can be broadcast, accepted and confirmed on-chain immediately. Tx 2 cannot be broadcast and accepted until lock_height has passed. So if Alice only knows K 3 and does not know Key 1 or Key 2 , then Out 1 can only be spent by Alice after lock_height has passed. If Bob on the other hand knows Key 2 then Out 1 can be spent by Bob immediately. We have a conditional timelock on Out 1 (confirmed, on-chain) where it can be spent either with Key 3 (after lock_height), or Key 2 immediately.","title":"Conditional Output Timelocks"},{"location":"technical/miscellaneous/contracts/#relative-conditional-output-timelocks","text":"Analogous to Bitcoin CheckSequenceVerify . By combining \"Conditional Timelock on Output\" with \"(Relative) Timelocked Transactions\" we can encumber a confirmed output with a relative timelock (relative to a related tx kernel). Tx 1 (containing Out 1 ) can be broadcast, accepted and confirmed on-chain immediately. Tx 2 cannot be broadcast and accepted until the relative lock_height has passed, relative to the referenced kernel from the earlier Tx 1 .","title":"(Relative) Conditional Output Timelocks"},{"location":"technical/miscellaneous/contracts/#atomic-swap","text":"This setup can work on Bitcoin, Ethereum and likely other chains. It relies on a time locked contract combined with a check for 2 public keys. On Bitcoin this would be a 2-of-2 multisig, one public key being Alice's, the second being the hash of a preimage that Bob has to reveal. In this setup, we consider public key derivation x*G to be the hash function and by Bob revealing x , Alice can then produce an adequate signature proving she knows x (in addition to her own private key). Alice has grins and Bob has bitcoin. They would like to swap. We assume Bob created an output on the Bitcoin blockchain that allows spending either by Alice if she learns a hash pre-image x , or by Bob after time Tb . Alice is ready to send her grins to Bob if he reveals x . First, Alice sends her grins to a multiparty timelock contract with a refund time Ta < Tb . To send the 2-of-2 output to Bob and execute the swap, Alice and Bob start as if they were building a normal trustless transaction as specified in section 2.1. Alice picks a random nonce ks and her blinding sum rs and sends ks*G and rs*G to Bob. Bob picks a random blinding factor rr and a random nonce kr . However this time, instead of simply sending sr = kr + e * rr with his rr*G and kr*G , Bob sends sr' = kr + x + e * rr as well as x*G . Alice can validate that sr'*G = kr*G + x*G + rr*G . She can also check that Bob has money locked with x*G on the other chain. Alice sends back her ss = ks + e * xs as she normally would, now that she can also compute e = SHA256(M | ks*G + kr*G) . To complete the signature, Bob computes sr = kr + e * rr and the final signature is (sr + ss, kr*G + ks*G) . As soon as Bob broadcasts the final transaction to get his new grins, Alice can compute sr' - sr to get x .","title":"Atomic Swap"},{"location":"technical/miscellaneous/contracts/#notes-on-the-bitcoin-setup","text":"Prior to completing the atomic swap, Bob needs to know Alice's public key. Bob would then create an output on the Bitcoin blockchain with a 2-of-2 multisig similar to alice_pubkey secret_pubkey 2 OP_CHECKMULTISIG . This should be wrapped in an OP_IF so Bob can get his money back after an agreed-upon time and all of this can even be wrapped in a P2SH. Here secret_pubkey is x*G from the previous section. To verify the output, Alice would take x*G , recreate the bitcoin script, hash it and check that her hash matches what's in the P2SH (step 2 in previous section). Once she gets x (step 6), she can build the 2 signatures necessary to spend the 2-of-2, having both private keys, and get her bitcoin.","title":"Notes on the Bitcoin setup"},{"location":"technical/miscellaneous/contracts/#relative-timelocks-lightning-network","text":"See No Recent Duplicate (NRD) transaction kernels RFC for more details.","title":"\"Relative Timelocks\" (Lightning Network)"},{"location":"technical/miscellaneous/dandelion/","text":"Dandelion++ in Grin: Privacy-Preserving Transaction Aggregation and Propagation Introduction The Dandelion++ protocol for broadcasting transactions, proposed by Fanti et al. (Sigmetrics 2018)[1], intends to defend against deanonymization attacks during transaction propagation. In Grin, it also provides an opportunity to aggregate transactions before they are broadcasted to the entire network. This document describes the protocol and the simplified version of it that is implemented in Grin. In the following section, past research on the protocol is summarized. This is then followed by describing details of the Grin implementation; the objectives behind its inclusion, how the current implementation differs from the original paper, what some of the known limitations are, and outlining some areas of improvement for future work. Previous research The original version of Dandelion was introduced by Fanti et al. and presented at ACM Sigmetrics 2017 [2]. On June 2017, a BIP [3] was proposed introducing a more practical and robust variant of Dandelion called Dandelion++, which was formalized into a paper in 2018. [1] The protocols are outlined at a high level here. For a more in-depth presentation with extensive literature references, please refer to the original papers. Motivation Dandelion was conceived as a way to mitigate against large scale deanonymization attacks on the network layer of Bitcoin, made possible by the diffusion method for propagating transactions on the network. By deploying \"super-nodes\" that connect to a large number of honest nodes on the network, adversaries can listen to the transactions relayed by the honest nodes as they get diffused symmetrically on the network using epidemic flooding or diffusion. By observing the spreading dynamic of a transaction, it has been proven possible to link it (and therefore also the sender's Bitcoin address) to the originating IP address with a high degree of accuracy, and as a result deanonymize users. Original Dandelion In the original paper [2], a dandelion spreading protocol is introduced. Dandelion spreading propagation consists of two phases: first the anonymity phase, or the \u201cstem\u201d phase, and second the spreading phase, or the \u201cfluff\u201d phase, as illustrated in Figure 1. Figure 1. Dandelion phase illustration. \u250c-> F ... \u250c-> D --\u2524 | \u2514-> G ... A --[stem]--> B --[stem]--> C --[fluff]--\u2524 | \u250c-> H ... \u2514-> E --\u2524 \u2514-> I ... In the initial stem-phase , each node relays the transaction to a single randomly selected peer , constructing a line graph. Users then forward transactions along the same path on the graph. After a random number of hops along the single stem, the transaction enters the fluff-phase , which behaves like ordinary diffusion. This means that even when an attacker can identify the originator of the fluff phase, it becomes more difficult to identify the source of the stem (and thus the original broadcaster of the transaction). The constructed line graph is periodically re-generated randomly, at the expiry of each epoch , limiting an adversary's possibility to build knowledge of graph. Epochs are asynchronous, with each individual node keeping its own internal clock and starting a new epoch once a certain threshold has been reached. The 'dandelion' name is derived from how the protocol resembles the spreading of the seeds of a dandelion. Dandelion++ In the Dandelion++ paper[1], the authors build on the original concept further, by defending against stronger adversaries that are allowed to disobey protocol. The original paper makes three idealistic assumptions: 1. All nodes obey protocol; 2. Each node generates exactly one transaction; and 3. All nodes on the network run Dandelion. An adversary can violate these rules, and by doing so break some of the anonymity properties. The modified Dandelion++ protocol makes small changes to most of the Dandelion choices, resulting in an exponentially more complex information space. This in turn makes it harder for an adversary to deanonymize the network. The paper describes five types of attacks, and proposes specific updates to the original Dandelion protocol to mitigate against these, presented in Table A (here in summarized form). Table A. Summary of Dandelion++ changes Attack Solution Graph-learning 4-regular anonymity graph Intersection Pseudorandom forwarding Graph-construction Non-interactive construction Black-hole Random stem timers Partial deployment Blind stem selection The Dandelion++ algorithm As with the original Dandelion protocol epochs are asynchronous, each node keeping track of its own epoch, which the suggested duration being in the order of 10 minutes. 1. Anonymity Graph Rather than a line graph as per the original paper (which is 2-regular), a quasi-4-regular graph (Figure 2) is constructed by a node at the beginning of each epoch: the node chooses (up to) two of its outbound edges uniformly at random as its dandelion++ relays . As a node enters into a new epoch, new dandelion++ relays are chosen. Figure 2. A 4-regular graph. in1 out1 \\ / \\ / NodeX / \\ / \\ in2 out2 NodeX has four connections to other nodes, input nodes in1 and in2 , and output nodes out1 and out2 . Note on using 4-regular vs 2-regular graphs The choice between using 4-regular or 2-regular (line) graphs is not obvious. The authors note that it is difficult to construct an exact 4-regular graph within a fully-distributed network in practice. They outline a method to construct an approximate 4-regular graph in the paper. They also write: [...] We recommend making the design decision between 4-regular graphs and line graphs based on the priorities of the system builders. If linkability of transactions is a first-order concern, then line graphs may be a better choice. Otherwise, we find that 4-regular graphs can give constant- order privacy benefits against adversaries with knowledge of the graph. 2. Transaction forwarding (own) At the beginning of each epoch, NodeX picks one of out1 and out2 to use as a route to broadcast its own transactions through as a stem-phase transaction. The same route is used throughout the duration epoch, and NodeX always forwards (stems) its own transaction. 3. Transaction forwarding (relay) At the start of each epoch, NodeX makes a choice to be either in fluff-mode or in stem-mode. This choice is made in pseudorandom fashion, with the paper suggesting it being computed from a hash of the node's own identity and epoch number. The probability of choosing to be in fluff-mode (or as the paper calls it, the path length parameter q ) is recommended to be q \u2264 0.2. Once the choice has been made whether to stem or to fluff, it applies to all relayed transactions during the epoch. If NodeX is in fluff-mode , it will broadcast any received transactions to the network using diffusion. If NodeX is in stem-mode , then at the beginning of each epoch it will map in1 to either out1 or out2 pseudorandomly, and similarly map in2 to either out1 or out2 in the same fashion. Based on this mapping, it will then forward all txs from in1 along the chosen route, and similarly forward all transactions from in2 along that route. The mapping persists throughout the duration of the epoch. 4. Fail-safe mechanism For each stem-phase transaction that was sent or relayed, NodeX tracks whether it is seen again as a fluff-phase transaction within some random amount of time. If not, the node fluffs the transaction itself. This expiration timer is set by each stem-node upon receiving a transaction to forward, and is chosen randomly. Nodes are initialized with a timeout parameter T base . As per equation (7) in the paper, when a stem-node v receives a transaction, it sets an expiration time T out (v): T out (v) ~ current_time + exp(1/T base ) If the transaction is not received again by relay v before the expiry of T out (v), it broadcasts the message using diffusion. This approach means that the first stem-node to broadcast is approximately uniformly selected among all stem-nodes who have seen the message, rather than the originating node. The paper also proceeds to specify the size of the initiating time out parameter T base as part of Proposition 3 in the paper: Proposition3. For a timeout parameter T base \u2265 (\u2212k(k\u22121)\u03b4 hop ) / 2 log(1\u2212\u03b5 ), where k , \u03b5 are parameters and \u03b4 hop is the time between each hop (e.g., network and/or internal node latency), transactions travel for k hops without any peer initiating diffusion with a probability of at least 1 \u2212 \u03b5 . Dandelion in Grin Objectives There are two main motives behind why Dandelion is included in Grin: Act as a countermeasure against mass de-anonymization attacks. Similar to Bitcoin, the Grin P2P network would be vulnerable to attackers deploying malicious \"super-nodes\" connecting to most peers on the network and monitoring transactions as they become diffused by their honest peers. This would allow a motivated actor to infer with a high degree of probability from which peer (IP address) transactions originate from, having negative privacy consequences. Aggregate transactions before they are being broadcasted to the entire network. This is a benefit to blockchains that enable non-interactive CoinJoins on the protocol level, such as Mimblewimble. Despite its good privacy features, some input and output linking is still possible in Mimblewimble and Grin.[4] If you know which input spends to which output, it is possible to construct a (very limited) transaction graph and follow a chain of transaction outputs (TXOs) as they are being spent. Aggregating transactions make this more difficult to carry out, as it becomes less clear which input spends to which output (Figure 3). In order for this to be effective, there needs to be a large anonymity set, i.e. many transactions to aggregate a transaction with. Dandelion enables this aggregation to occur before transactions are fluffed and diffused to the entire network. This adds obfuscation to the transaction graph, as a malicious observer who is not participating in the stemming or fluffing would not only need to figure out from where a transaction originated, but also which TXOs out of a larger group should be attributed to the originating transaction. Figure 3. Aggregating transactions 3.1 Transactions (not aggregated) --------------------------------------------- TX1 INPUT_A ______________ OUTPUT_X |_____ OUTPUT_Y KERNEL 1 --------------------------------------------- TX2 INPUT_B ______________ OUTPUT_Z INPUT_C ________| KERNEL 2 --------------------------------------------- 3.2 Transactions (aggregated) --------------------------------------------- TX1+2 INPUT_A ______________ OUTPUT_X INPUT_B ________|_____ OUTPUT_Y INPUT_C ________|_____ OUTPUT_Z KERNEL 1 KERNEL 2 --------------------------------------------- Current implementation Grin implements a simplified version of the Dandelion++ protocol. It's been improved several times, most recently in version 1.1.0 [5]. DandelionEpoch tracks a node's current epoch. This is configurable via epoch_secs with default epoch set to last for 10 minutes. Epochs are set and tracked by nodes individually. At the beginning of an epoch, the node chooses a single connected peer at random to use as their outbound relay. At the beginning of an epoch, the node makes a decision whether to be in stem mode or in fluff mode. This decision lasts for the duration of the epoch. By default, this is a random choice, with the probability to be in stem mode set to 90%, which implies a fluff mode probability, q of 10%. The probability is configurable via DANDELION_STEM_PROBABILITY . The number of expected stem hops a transaction does before arriving to a fluff node is 1/q = 1/0.1 = 10 . Any transactions received from inbound connected nodes or transactions originated from the node itself are first added to the node's stempool , which is a list of stem transactions, that each node keeps track of individually. Transactions are removed from the stempool if: The node fluffs the transaction itself. The node sees the transaction in question propagated through regular diffusion, i.e. from a different peer having \"fluffed\" it. The node receives a block containing this transaction, meaning that the transaction was propagated and included in a block. For each transaction added to the stempool, the node sets an embargo timer . This is set by default to 180 seconds, and is configurable via DANDELION_EMBARGO_SECS . Regardless of whether the node is in fluff or stem mode, any transactions generated from the node itself are forwarded onwards to their relay node as a stem transaction.[6] A dandelion_monitor runs every 10 seconds and handles tasks. If the node is in stem mode , then: After being added to the stempool, received stem transactions are forwarded onto the their relay node as a stem transaction. As peers connect at random, it is possible they create a circular loop of connected stem mode nodes (i.e. A -> B -> C -> A ). Therefore, if a node receives a stem transaction from an inbound node that already exists in its own stempool, it will fluff it, broadcasting it using regular diffusion. dandelion_monitor checks for transactions in the node's stempool with an expired embargo timer, and broadcast those individually. If the node is in fluff mode , then: Transactions received from inbound nodes are kept in the stempool. dandelion_monitor checks in the stempool whether any transactions are older than 30 seconds (configurable as DANDELION_AGGREGATION_SECS ). If so, these are aggregated and then fluffed. Otherwise no action is taken, allowing for more stem transactions to aggregate in the stempool in time for the next triggering of dandelion_monitor . At the expiry of an epoch, all stem transactions remaining in the stem pool are aggregated and fluffed. Known limitations 2-regular graphs are used rather than 4-regular graphs as proposed by the paper. It's not clear what impact this has, the paper suggests a trade-off between general linkability of transactions and protection against adversaries who know the entire network graph. Unlike the Dandelion++ paper, the embargo timer is by default identical across all nodes. This means that during a black-hole attack where a malicious node withholds transactions, the node most likely to have its embargo timer expire and fluff the transaction will be the originating node, therefore exposing itself. Future work Randomized embargo timer according to the recommendations of the paper to make it more random which node fluffs an expired transaction. Evaluation of whether 4-regular graphs are preferred over 2-regular line graphs. Simulation of the current implementation to understand performance. Improved understanding of the benefits of transaction aggregation prior to fluffing. References [1] (Sigmetrics 2018) Dandelion++: Lightweight Cryptocurrency Networking with Formal Anonymity Guarantees [2] (Sigmetrics 2017) Dandelion: Redesigning the Bitcoin Network for Anonymity [3] Dandelion BIP [4] Grin Privacy Primer [5] #2628: Dandelion++ Rewrite [6] #2876: Always stem local txs if configured that way (unless explicitly fluffed)","title":"Dandelion"},{"location":"technical/miscellaneous/dandelion/#dandelion-in-grin-privacy-preserving-transaction-aggregation-and-propagation","text":"","title":"Dandelion++ in Grin: Privacy-Preserving Transaction Aggregation and Propagation"},{"location":"technical/miscellaneous/dandelion/#introduction","text":"The Dandelion++ protocol for broadcasting transactions, proposed by Fanti et al. (Sigmetrics 2018)[1], intends to defend against deanonymization attacks during transaction propagation. In Grin, it also provides an opportunity to aggregate transactions before they are broadcasted to the entire network. This document describes the protocol and the simplified version of it that is implemented in Grin. In the following section, past research on the protocol is summarized. This is then followed by describing details of the Grin implementation; the objectives behind its inclusion, how the current implementation differs from the original paper, what some of the known limitations are, and outlining some areas of improvement for future work.","title":"Introduction"},{"location":"technical/miscellaneous/dandelion/#previous-research","text":"The original version of Dandelion was introduced by Fanti et al. and presented at ACM Sigmetrics 2017 [2]. On June 2017, a BIP [3] was proposed introducing a more practical and robust variant of Dandelion called Dandelion++, which was formalized into a paper in 2018. [1] The protocols are outlined at a high level here. For a more in-depth presentation with extensive literature references, please refer to the original papers.","title":"Previous research"},{"location":"technical/miscellaneous/dandelion/#motivation","text":"Dandelion was conceived as a way to mitigate against large scale deanonymization attacks on the network layer of Bitcoin, made possible by the diffusion method for propagating transactions on the network. By deploying \"super-nodes\" that connect to a large number of honest nodes on the network, adversaries can listen to the transactions relayed by the honest nodes as they get diffused symmetrically on the network using epidemic flooding or diffusion. By observing the spreading dynamic of a transaction, it has been proven possible to link it (and therefore also the sender's Bitcoin address) to the originating IP address with a high degree of accuracy, and as a result deanonymize users.","title":"Motivation"},{"location":"technical/miscellaneous/dandelion/#original-dandelion","text":"In the original paper [2], a dandelion spreading protocol is introduced. Dandelion spreading propagation consists of two phases: first the anonymity phase, or the \u201cstem\u201d phase, and second the spreading phase, or the \u201cfluff\u201d phase, as illustrated in Figure 1. Figure 1. Dandelion phase illustration. \u250c-> F ... \u250c-> D --\u2524 | \u2514-> G ... A --[stem]--> B --[stem]--> C --[fluff]--\u2524 | \u250c-> H ... \u2514-> E --\u2524 \u2514-> I ... In the initial stem-phase , each node relays the transaction to a single randomly selected peer , constructing a line graph. Users then forward transactions along the same path on the graph. After a random number of hops along the single stem, the transaction enters the fluff-phase , which behaves like ordinary diffusion. This means that even when an attacker can identify the originator of the fluff phase, it becomes more difficult to identify the source of the stem (and thus the original broadcaster of the transaction). The constructed line graph is periodically re-generated randomly, at the expiry of each epoch , limiting an adversary's possibility to build knowledge of graph. Epochs are asynchronous, with each individual node keeping its own internal clock and starting a new epoch once a certain threshold has been reached. The 'dandelion' name is derived from how the protocol resembles the spreading of the seeds of a dandelion.","title":"Original Dandelion"},{"location":"technical/miscellaneous/dandelion/#dandelion","text":"In the Dandelion++ paper[1], the authors build on the original concept further, by defending against stronger adversaries that are allowed to disobey protocol. The original paper makes three idealistic assumptions: 1. All nodes obey protocol; 2. Each node generates exactly one transaction; and 3. All nodes on the network run Dandelion. An adversary can violate these rules, and by doing so break some of the anonymity properties. The modified Dandelion++ protocol makes small changes to most of the Dandelion choices, resulting in an exponentially more complex information space. This in turn makes it harder for an adversary to deanonymize the network. The paper describes five types of attacks, and proposes specific updates to the original Dandelion protocol to mitigate against these, presented in Table A (here in summarized form). Table A. Summary of Dandelion++ changes Attack Solution Graph-learning 4-regular anonymity graph Intersection Pseudorandom forwarding Graph-construction Non-interactive construction Black-hole Random stem timers Partial deployment Blind stem selection","title":"Dandelion++"},{"location":"technical/miscellaneous/dandelion/#the-dandelion-algorithm","text":"As with the original Dandelion protocol epochs are asynchronous, each node keeping track of its own epoch, which the suggested duration being in the order of 10 minutes.","title":"The Dandelion++ algorithm"},{"location":"technical/miscellaneous/dandelion/#1-anonymity-graph","text":"Rather than a line graph as per the original paper (which is 2-regular), a quasi-4-regular graph (Figure 2) is constructed by a node at the beginning of each epoch: the node chooses (up to) two of its outbound edges uniformly at random as its dandelion++ relays . As a node enters into a new epoch, new dandelion++ relays are chosen. Figure 2. A 4-regular graph. in1 out1 \\ / \\ / NodeX / \\ / \\ in2 out2 NodeX has four connections to other nodes, input nodes in1 and in2 , and output nodes out1 and out2 . Note on using 4-regular vs 2-regular graphs The choice between using 4-regular or 2-regular (line) graphs is not obvious. The authors note that it is difficult to construct an exact 4-regular graph within a fully-distributed network in practice. They outline a method to construct an approximate 4-regular graph in the paper. They also write: [...] We recommend making the design decision between 4-regular graphs and line graphs based on the priorities of the system builders. If linkability of transactions is a first-order concern, then line graphs may be a better choice. Otherwise, we find that 4-regular graphs can give constant- order privacy benefits against adversaries with knowledge of the graph.","title":"1. Anonymity Graph"},{"location":"technical/miscellaneous/dandelion/#2-transaction-forwarding-own","text":"At the beginning of each epoch, NodeX picks one of out1 and out2 to use as a route to broadcast its own transactions through as a stem-phase transaction. The same route is used throughout the duration epoch, and NodeX always forwards (stems) its own transaction.","title":"2. Transaction forwarding (own)"},{"location":"technical/miscellaneous/dandelion/#3-transaction-forwarding-relay","text":"At the start of each epoch, NodeX makes a choice to be either in fluff-mode or in stem-mode. This choice is made in pseudorandom fashion, with the paper suggesting it being computed from a hash of the node's own identity and epoch number. The probability of choosing to be in fluff-mode (or as the paper calls it, the path length parameter q ) is recommended to be q \u2264 0.2. Once the choice has been made whether to stem or to fluff, it applies to all relayed transactions during the epoch. If NodeX is in fluff-mode , it will broadcast any received transactions to the network using diffusion. If NodeX is in stem-mode , then at the beginning of each epoch it will map in1 to either out1 or out2 pseudorandomly, and similarly map in2 to either out1 or out2 in the same fashion. Based on this mapping, it will then forward all txs from in1 along the chosen route, and similarly forward all transactions from in2 along that route. The mapping persists throughout the duration of the epoch.","title":"3. Transaction forwarding (relay)"},{"location":"technical/miscellaneous/dandelion/#4-fail-safe-mechanism","text":"For each stem-phase transaction that was sent or relayed, NodeX tracks whether it is seen again as a fluff-phase transaction within some random amount of time. If not, the node fluffs the transaction itself. This expiration timer is set by each stem-node upon receiving a transaction to forward, and is chosen randomly. Nodes are initialized with a timeout parameter T base . As per equation (7) in the paper, when a stem-node v receives a transaction, it sets an expiration time T out (v): T out (v) ~ current_time + exp(1/T base ) If the transaction is not received again by relay v before the expiry of T out (v), it broadcasts the message using diffusion. This approach means that the first stem-node to broadcast is approximately uniformly selected among all stem-nodes who have seen the message, rather than the originating node. The paper also proceeds to specify the size of the initiating time out parameter T base as part of Proposition 3 in the paper: Proposition3. For a timeout parameter T base \u2265 (\u2212k(k\u22121)\u03b4 hop ) / 2 log(1\u2212\u03b5 ), where k , \u03b5 are parameters and \u03b4 hop is the time between each hop (e.g., network and/or internal node latency), transactions travel for k hops without any peer initiating diffusion with a probability of at least 1 \u2212 \u03b5 .","title":"4. Fail-safe mechanism"},{"location":"technical/miscellaneous/dandelion/#dandelion-in-grin","text":"","title":"Dandelion in Grin"},{"location":"technical/miscellaneous/dandelion/#objectives","text":"There are two main motives behind why Dandelion is included in Grin: Act as a countermeasure against mass de-anonymization attacks. Similar to Bitcoin, the Grin P2P network would be vulnerable to attackers deploying malicious \"super-nodes\" connecting to most peers on the network and monitoring transactions as they become diffused by their honest peers. This would allow a motivated actor to infer with a high degree of probability from which peer (IP address) transactions originate from, having negative privacy consequences. Aggregate transactions before they are being broadcasted to the entire network. This is a benefit to blockchains that enable non-interactive CoinJoins on the protocol level, such as Mimblewimble. Despite its good privacy features, some input and output linking is still possible in Mimblewimble and Grin.[4] If you know which input spends to which output, it is possible to construct a (very limited) transaction graph and follow a chain of transaction outputs (TXOs) as they are being spent. Aggregating transactions make this more difficult to carry out, as it becomes less clear which input spends to which output (Figure 3). In order for this to be effective, there needs to be a large anonymity set, i.e. many transactions to aggregate a transaction with. Dandelion enables this aggregation to occur before transactions are fluffed and diffused to the entire network. This adds obfuscation to the transaction graph, as a malicious observer who is not participating in the stemming or fluffing would not only need to figure out from where a transaction originated, but also which TXOs out of a larger group should be attributed to the originating transaction. Figure 3. Aggregating transactions 3.1 Transactions (not aggregated) --------------------------------------------- TX1 INPUT_A ______________ OUTPUT_X |_____ OUTPUT_Y KERNEL 1 --------------------------------------------- TX2 INPUT_B ______________ OUTPUT_Z INPUT_C ________| KERNEL 2 --------------------------------------------- 3.2 Transactions (aggregated) --------------------------------------------- TX1+2 INPUT_A ______________ OUTPUT_X INPUT_B ________|_____ OUTPUT_Y INPUT_C ________|_____ OUTPUT_Z KERNEL 1 KERNEL 2 ---------------------------------------------","title":"Objectives"},{"location":"technical/miscellaneous/dandelion/#current-implementation","text":"Grin implements a simplified version of the Dandelion++ protocol. It's been improved several times, most recently in version 1.1.0 [5]. DandelionEpoch tracks a node's current epoch. This is configurable via epoch_secs with default epoch set to last for 10 minutes. Epochs are set and tracked by nodes individually. At the beginning of an epoch, the node chooses a single connected peer at random to use as their outbound relay. At the beginning of an epoch, the node makes a decision whether to be in stem mode or in fluff mode. This decision lasts for the duration of the epoch. By default, this is a random choice, with the probability to be in stem mode set to 90%, which implies a fluff mode probability, q of 10%. The probability is configurable via DANDELION_STEM_PROBABILITY . The number of expected stem hops a transaction does before arriving to a fluff node is 1/q = 1/0.1 = 10 . Any transactions received from inbound connected nodes or transactions originated from the node itself are first added to the node's stempool , which is a list of stem transactions, that each node keeps track of individually. Transactions are removed from the stempool if: The node fluffs the transaction itself. The node sees the transaction in question propagated through regular diffusion, i.e. from a different peer having \"fluffed\" it. The node receives a block containing this transaction, meaning that the transaction was propagated and included in a block. For each transaction added to the stempool, the node sets an embargo timer . This is set by default to 180 seconds, and is configurable via DANDELION_EMBARGO_SECS . Regardless of whether the node is in fluff or stem mode, any transactions generated from the node itself are forwarded onwards to their relay node as a stem transaction.[6] A dandelion_monitor runs every 10 seconds and handles tasks. If the node is in stem mode , then: After being added to the stempool, received stem transactions are forwarded onto the their relay node as a stem transaction. As peers connect at random, it is possible they create a circular loop of connected stem mode nodes (i.e. A -> B -> C -> A ). Therefore, if a node receives a stem transaction from an inbound node that already exists in its own stempool, it will fluff it, broadcasting it using regular diffusion. dandelion_monitor checks for transactions in the node's stempool with an expired embargo timer, and broadcast those individually. If the node is in fluff mode , then: Transactions received from inbound nodes are kept in the stempool. dandelion_monitor checks in the stempool whether any transactions are older than 30 seconds (configurable as DANDELION_AGGREGATION_SECS ). If so, these are aggregated and then fluffed. Otherwise no action is taken, allowing for more stem transactions to aggregate in the stempool in time for the next triggering of dandelion_monitor . At the expiry of an epoch, all stem transactions remaining in the stem pool are aggregated and fluffed.","title":"Current implementation"},{"location":"technical/miscellaneous/dandelion/#known-limitations","text":"2-regular graphs are used rather than 4-regular graphs as proposed by the paper. It's not clear what impact this has, the paper suggests a trade-off between general linkability of transactions and protection against adversaries who know the entire network graph. Unlike the Dandelion++ paper, the embargo timer is by default identical across all nodes. This means that during a black-hole attack where a malicious node withholds transactions, the node most likely to have its embargo timer expire and fluff the transaction will be the originating node, therefore exposing itself.","title":"Known limitations"},{"location":"technical/miscellaneous/dandelion/#future-work","text":"Randomized embargo timer according to the recommendations of the paper to make it more random which node fluffs an expired transaction. Evaluation of whether 4-regular graphs are preferred over 2-regular line graphs. Simulation of the current implementation to understand performance. Improved understanding of the benefits of transaction aggregation prior to fluffing.","title":"Future work"},{"location":"technical/miscellaneous/dandelion/#references","text":"[1] (Sigmetrics 2018) Dandelion++: Lightweight Cryptocurrency Networking with Formal Anonymity Guarantees [2] (Sigmetrics 2017) Dandelion: Redesigning the Bitcoin Network for Anonymity [3] Dandelion BIP [4] Grin Privacy Primer [5] #2628: Dandelion++ Rewrite [6] #2876: Always stem local txs if configured that way (unless explicitly fluffed)","title":"References"},{"location":"technical/miscellaneous/range-proof-format/","text":"Range Proof Format secp256k1-zkp Bulletproof format Grin uses Bulletproofs range proofs. The currently most efficient range proofs which do not require a trusted setup. Grin uses mimblewimble/secp256k1-zkp , a fork of the Blockstream C library ElementsProject/secp256k1-zkp , which has Bulletproofs implemented but not yet merged into the main branch. In this document I will explain how to recover the proof parameters from the output bytes. Note that I will name the parameters as they are named in the C library, which might be slightly different then to the paper in some cases. Here is a sample proof output taken from a Grin transaction: 0b1bdf235e9c438aab5c6d02d3fe8173304bc528a3330825fb2311fa60fcdd6bfb92a248e26f849aebd511d2b326fa34b7f3030517d2f8e08a9b3cac7fa9fd200d07a46ca6ec5af30ce569b1e5faf2acf525cf1ed90cbed74ab7378b9b3957f28635fe7440aac2dc2c4bf43265b6ad1bfa82fddd9a827c4e97a913ce451b9a66bb06d3c08e03e85e98c581bbdf8c852796371a4603b8d52b80a1f2e95bd5e2a91c7a00b4d4564d9586235a7858d9ce8a8888bead7d51be2dd802de5af2921e079586817fce16d36c7764af8b4bf133b56b39970d6a568bf9ff101e6d33409e7c3bb081df7425b276655be611941245ceaad529495a86bc0e3d0f8634a8acf65c34c4e244959a5098bd58285408945a247d2fd894e5b18027d698c7e494e4256110553df54babf90592fbdffa0138b6b5a2a423ea5e2ec4d8f852a33c271a73b10fed9e1ee8cb2db1e71311cacd9e1d0b6dbf6cfab15723ec3cac4cc52154fc9d532202a085238e756ad1fa804cce2a634decc1b348f6ff939f9f80187d85aa5c308224a505c75e7f58fc7f35424276db7956474c1895e23ac55f864f4177b59f3ce92ef8c99e011cf55e0cefc5635d2eaf573df29af057a19bb209392a8c0e29a4b77adad76d385422e7f1d06de2d4f14e61ac3619aa22ae5bc288bb41cb56ddb70bb39ae84d00eb0cb34b4063bb55a83b9fe52604e545adcd41beb6ce14cdff73a21beb7493fa443a34585b7d2927f608cad17aa5f0e8e154b14d35315f63dd3580e80d06d8be4039f58778967f7bf2cdd9020fbcc9fed799b8159814f6a261c568e8b59c59df3180efb9cc13c576bf313248c96fa867aba43a80e799ff19ac685d7208cea7944dc9dcba7a61f2809540ecd0711e76b601969bdc551845e0b11fb821871d00e417ad002a70353867db25fa647e98a0db4c3bbaf828d97fc66079ef0d First, we have two 32 byte scalars. These are the already negated versions of taux and mu. We negate them such that the verifier doesn't have to do it. ( rangeproof_impl.h 701-702 ) 0b1bdf235e9c438aab5c6d02d3fe8173304bc528a3330825fb2311fa60fcdd6b (5024686248162052924872973414517693136231035491146611931625298995470137089387) taux (negated) fb92a248e26f849aebd511d2b326fa34b7f3030517d2f8e08a9b3cac7fa9fd20 (113789604713728301456840843635921464549630649029317112794749678552821986360608) mu (negated) After that, we have 4 points, which represent commitments A, S, T1, T2. Points are encoded in a very smart way. We have one offset byte. We use this offset byte indicate the LSB of y, telling us if the points y value needs to be negated when recovered. If this is the case, the bit is set to 1 (starting at the LSB); otherwise, it is left at 0. If we have more then eight points, we need 2 bytes offset, if we have more then 16, then three, and so forth. ( rangeproof_impl.h 703 ) 0d offset 0000 1101 offset in binary From this, we can recover the 4 points. (I am using the standard compressed point version here with leading 02 or 03) We start reading at the LSB. 0307a46ca6ec5af30ce569b1e5faf2acf525cf1ed90cbed74ab7378b9b3957f286 A (03 because of the 1 bit in the offset) 0235fe7440aac2dc2c4bf43265b6ad1bfa82fddd9a827c4e97a913ce451b9a66bb S (02 here because of the 0 bit in the offset) 0306d3c08e03e85e98c581bbdf8c852796371a4603b8d52b80a1f2e95bd5e2a91c T1 037a00b4d4564d9586235a7858d9ce8a8888bead7d51be2dd802de5af2921e0795 T2 Next, we have the final value of the dot product which again is a 32-byte scalar ( inner_product_impl.h 811 ) 86817fce16d36c7764af8b4bf133b56b39970d6a568bf9ff101e6d33409e7c3b (60838727059453008536034129618950719358562694528830851223208761064459354405947) dot Then we have the final values (32-byte scalars) of the shrunk vectors a, b used in the inner product protocol. The library does not do the last round of the protocol, meaning it will stop when the vectors are of length two instead of length one. This is because every round creates two commitments Li and Ri. If we don't do the last round, we spare two commitments with the cost that our two vectors are of size two instead of one, which is more space-efficient, and we save computing time. ( inner_product_impl.h 835-836 ) b081df7425b276655be611941245ceaad529495a86bc0e3d0f8634a8acf65c34 (79836526842770413616887368822368313168206709119259360230886972660827215518772) a1 c4e244959a5098bd58285408945a247d2fd894e5b18027d698c7e494e4256110 (89053099110995010594661038229216983605420219413380810817771304480647904846096) b1 553df54babf90592fbdffa0138b6b5a2a423ea5e2ec4d8f852a33c271a73b10f (38556062768490931671602594328406809964645337276375001909352144464132590252303) a2 ed9e1ee8cb2db1e71311cacd9e1d0b6dbf6cfab15723ec3cac4cc52154fc9d53 (107477520278964342277912932357487306000871347661927764278313323679782451060051) b2 And last we have the commitments Li and Ri of every round. In Grin we create range proofs with a range of 0 to (2^64 -1). This means we have six rounds (log(64) = 6); however, since we stop early, we only do five rounds, so 10 points instead of 12. The implementation always computes L before R. ( inner_product_impl.h 627 ) Again we have an offset in which we specify how to recover y values. Now since we have more than eight points we need two bytes offset. We start reading at the LSB of the first byte and then go to the LSB of the second byte. ( inner_product_impl.h 839 ) 2202 offset 0010 0010 first offset byte (binary) 0000 0010 second offset byte (binary) 02a085238e756ad1fa804cce2a634decc1b348f6ff939f9f80187d85aa5c308224 L1 03a505c75e7f58fc7f35424276db7956474c1895e23ac55f864f4177b59f3ce92e R1 (03 because of the 1 at bit number 2 of first offset byte) 02f8c99e011cf55e0cefc5635d2eaf573df29af057a19bb209392a8c0e29a4b77a L2 02dad76d385422e7f1d06de2d4f14e61ac3619aa22ae5bc288bb41cb56ddb70bb3 R2 029ae84d00eb0cb34b4063bb55a83b9fe52604e545adcd41beb6ce14cdff73a21b L3 03eb7493fa443a34585b7d2927f608cad17aa5f0e8e154b14d35315f63dd3580e8 R3 (03 because of the 1 at bit number 6 of first offset byte) 020d06d8be4039f58778967f7bf2cdd9020fbcc9fed799b8159814f6a261c568e8 L4 02b59c59df3180efb9cc13c576bf313248c96fa867aba43a80e799ff19ac685d72 R4 0208cea7944dc9dcba7a61f2809540ecd0711e76b601969bdc551845e0b11fb821 L5 03871d00e417ad002a70353867db25fa647e98a0db4c3bbaf828d97fc66079ef0d R5 (03 because of the 1 at bit number 2 of second offset byte)","title":"Range Proofs Format"},{"location":"technical/miscellaneous/range-proof-format/#range-proof-format","text":"","title":"Range Proof Format"},{"location":"technical/miscellaneous/range-proof-format/#secp256k1-zkp-bulletproof-format","text":"Grin uses Bulletproofs range proofs. The currently most efficient range proofs which do not require a trusted setup. Grin uses mimblewimble/secp256k1-zkp , a fork of the Blockstream C library ElementsProject/secp256k1-zkp , which has Bulletproofs implemented but not yet merged into the main branch. In this document I will explain how to recover the proof parameters from the output bytes. Note that I will name the parameters as they are named in the C library, which might be slightly different then to the paper in some cases. Here is a sample proof output taken from a Grin transaction: 0b1bdf235e9c438aab5c6d02d3fe8173304bc528a3330825fb2311fa60fcdd6bfb92a248e26f849aebd511d2b326fa34b7f3030517d2f8e08a9b3cac7fa9fd200d07a46ca6ec5af30ce569b1e5faf2acf525cf1ed90cbed74ab7378b9b3957f28635fe7440aac2dc2c4bf43265b6ad1bfa82fddd9a827c4e97a913ce451b9a66bb06d3c08e03e85e98c581bbdf8c852796371a4603b8d52b80a1f2e95bd5e2a91c7a00b4d4564d9586235a7858d9ce8a8888bead7d51be2dd802de5af2921e079586817fce16d36c7764af8b4bf133b56b39970d6a568bf9ff101e6d33409e7c3bb081df7425b276655be611941245ceaad529495a86bc0e3d0f8634a8acf65c34c4e244959a5098bd58285408945a247d2fd894e5b18027d698c7e494e4256110553df54babf90592fbdffa0138b6b5a2a423ea5e2ec4d8f852a33c271a73b10fed9e1ee8cb2db1e71311cacd9e1d0b6dbf6cfab15723ec3cac4cc52154fc9d532202a085238e756ad1fa804cce2a634decc1b348f6ff939f9f80187d85aa5c308224a505c75e7f58fc7f35424276db7956474c1895e23ac55f864f4177b59f3ce92ef8c99e011cf55e0cefc5635d2eaf573df29af057a19bb209392a8c0e29a4b77adad76d385422e7f1d06de2d4f14e61ac3619aa22ae5bc288bb41cb56ddb70bb39ae84d00eb0cb34b4063bb55a83b9fe52604e545adcd41beb6ce14cdff73a21beb7493fa443a34585b7d2927f608cad17aa5f0e8e154b14d35315f63dd3580e80d06d8be4039f58778967f7bf2cdd9020fbcc9fed799b8159814f6a261c568e8b59c59df3180efb9cc13c576bf313248c96fa867aba43a80e799ff19ac685d7208cea7944dc9dcba7a61f2809540ecd0711e76b601969bdc551845e0b11fb821871d00e417ad002a70353867db25fa647e98a0db4c3bbaf828d97fc66079ef0d First, we have two 32 byte scalars. These are the already negated versions of taux and mu. We negate them such that the verifier doesn't have to do it. ( rangeproof_impl.h 701-702 ) 0b1bdf235e9c438aab5c6d02d3fe8173304bc528a3330825fb2311fa60fcdd6b (5024686248162052924872973414517693136231035491146611931625298995470137089387) taux (negated) fb92a248e26f849aebd511d2b326fa34b7f3030517d2f8e08a9b3cac7fa9fd20 (113789604713728301456840843635921464549630649029317112794749678552821986360608) mu (negated) After that, we have 4 points, which represent commitments A, S, T1, T2. Points are encoded in a very smart way. We have one offset byte. We use this offset byte indicate the LSB of y, telling us if the points y value needs to be negated when recovered. If this is the case, the bit is set to 1 (starting at the LSB); otherwise, it is left at 0. If we have more then eight points, we need 2 bytes offset, if we have more then 16, then three, and so forth. ( rangeproof_impl.h 703 ) 0d offset 0000 1101 offset in binary From this, we can recover the 4 points. (I am using the standard compressed point version here with leading 02 or 03) We start reading at the LSB. 0307a46ca6ec5af30ce569b1e5faf2acf525cf1ed90cbed74ab7378b9b3957f286 A (03 because of the 1 bit in the offset) 0235fe7440aac2dc2c4bf43265b6ad1bfa82fddd9a827c4e97a913ce451b9a66bb S (02 here because of the 0 bit in the offset) 0306d3c08e03e85e98c581bbdf8c852796371a4603b8d52b80a1f2e95bd5e2a91c T1 037a00b4d4564d9586235a7858d9ce8a8888bead7d51be2dd802de5af2921e0795 T2 Next, we have the final value of the dot product which again is a 32-byte scalar ( inner_product_impl.h 811 ) 86817fce16d36c7764af8b4bf133b56b39970d6a568bf9ff101e6d33409e7c3b (60838727059453008536034129618950719358562694528830851223208761064459354405947) dot Then we have the final values (32-byte scalars) of the shrunk vectors a, b used in the inner product protocol. The library does not do the last round of the protocol, meaning it will stop when the vectors are of length two instead of length one. This is because every round creates two commitments Li and Ri. If we don't do the last round, we spare two commitments with the cost that our two vectors are of size two instead of one, which is more space-efficient, and we save computing time. ( inner_product_impl.h 835-836 ) b081df7425b276655be611941245ceaad529495a86bc0e3d0f8634a8acf65c34 (79836526842770413616887368822368313168206709119259360230886972660827215518772) a1 c4e244959a5098bd58285408945a247d2fd894e5b18027d698c7e494e4256110 (89053099110995010594661038229216983605420219413380810817771304480647904846096) b1 553df54babf90592fbdffa0138b6b5a2a423ea5e2ec4d8f852a33c271a73b10f (38556062768490931671602594328406809964645337276375001909352144464132590252303) a2 ed9e1ee8cb2db1e71311cacd9e1d0b6dbf6cfab15723ec3cac4cc52154fc9d53 (107477520278964342277912932357487306000871347661927764278313323679782451060051) b2 And last we have the commitments Li and Ri of every round. In Grin we create range proofs with a range of 0 to (2^64 -1). This means we have six rounds (log(64) = 6); however, since we stop early, we only do five rounds, so 10 points instead of 12. The implementation always computes L before R. ( inner_product_impl.h 627 ) Again we have an offset in which we specify how to recover y values. Now since we have more than eight points we need two bytes offset. We start reading at the LSB of the first byte and then go to the LSB of the second byte. ( inner_product_impl.h 839 ) 2202 offset 0010 0010 first offset byte (binary) 0000 0010 second offset byte (binary) 02a085238e756ad1fa804cce2a634decc1b348f6ff939f9f80187d85aa5c308224 L1 03a505c75e7f58fc7f35424276db7956474c1895e23ac55f864f4177b59f3ce92e R1 (03 because of the 1 at bit number 2 of first offset byte) 02f8c99e011cf55e0cefc5635d2eaf573df29af057a19bb209392a8c0e29a4b77a L2 02dad76d385422e7f1d06de2d4f14e61ac3619aa22ae5bc288bb41cb56ddb70bb3 R2 029ae84d00eb0cb34b4063bb55a83b9fe52604e545adcd41beb6ce14cdff73a21b L3 03eb7493fa443a34585b7d2927f608cad17aa5f0e8e154b14d35315f63dd3580e8 R3 (03 because of the 1 at bit number 6 of first offset byte) 020d06d8be4039f58778967f7bf2cdd9020fbcc9fed799b8159814f6a261c568e8 L4 02b59c59df3180efb9cc13c576bf313248c96fa867aba43a80e799ff19ac685d72 R4 0208cea7944dc9dcba7a61f2809540ecd0711e76b601969bdc551845e0b11fb821 L5 03871d00e417ad002a70353867db25fa647e98a0db4c3bbaf828d97fc66079ef0d R5 (03 because of the 1 at bit number 2 of second offset byte)","title":"secp256k1-zkp Bulletproof format"}]}